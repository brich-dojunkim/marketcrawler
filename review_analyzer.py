# ì™„ì„±ëœ í•œêµ­ì–´ ìƒí’ˆ ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ
# ì¿ íŒ¡ ë¦¬ë·° ë°ì´í„° ë¶„ì„ ë° ì—‘ì…€ ê²°ê³¼ ì¶œë ¥

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
from datetime import datetime
import os
warnings.filterwarnings('ignore')

# í•œêµ­ì–´ ìì—°ì–´ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
# KoNLPy ì„í¬íŠ¸ ì‹œ Java ì˜¤ë¥˜ ì²˜ë¦¬
try:
    from konlpy.tag import Okt, Kkma, Komoran
    KONLPY_AVAILABLE = True
    print("KoNLPy ì‚¬ìš© ê°€ëŠ¥")
except Exception as e:
    print(f"KoNLPy ì‚¬ìš© ë¶ˆê°€: {e}")
    print("Javaê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜ ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    KONLPY_AVAILABLE = False

from kiwipiepy import Kiwi

# PyKoSpacing ì„í¬íŠ¸ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬ (TensorFlow ì˜ì¡´ì„±)
try:
    from pykospacing import Spacing
    PYKOSPACING_AVAILABLE = True
    print("PyKoSpacing ì‚¬ìš© ê°€ëŠ¥")
except ImportError as e:
    print(f"PyKoSpacing ì‚¬ìš© ë¶ˆê°€: {e}")
    print("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install tensorflow' ì„¤ì¹˜ í›„ ì¬ì‹œë„í•˜ì„¸ìš”.")
    PYKOSPACING_AVAILABLE = False

# ë¹„ì§€ë„ í•™ìŠµ ê¸°ë°˜
from soynlp.word import WordExtractor
from soynlp.tokenizer import LTokenizer
from krwordrank.word import KRWordRank

# ë”¥ëŸ¬ë‹ ê¸°ë°˜ (KoBERT) - ì„ íƒì  ì„í¬íŠ¸
try:
    import torch
    from transformers import BertTokenizer, BertForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
    print("PyTorch/Transformers ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    print("PyTorch/Transformersë¥¼ ì„¤ì¹˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    TRANSFORMERS_AVAILABLE = False

# ì‹œê°í™” ë° ë¶„ì„
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation

# =============================================================================
# ReviewAnalyzer í´ë˜ìŠ¤
# =============================================================================

class ReviewAnalyzer:
    """ìƒí’ˆ ë¦¬ë·° ë¶„ì„ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        # ì „ì—­ ë³€ìˆ˜ ì°¸ì¡°
        global KONLPY_AVAILABLE
        
        # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.kiwi = Kiwi()  # KiwiëŠ” Java ë¶ˆí•„ìš”
        
        # KoNLPy ì´ˆê¸°í™” (Java ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if KONLPY_AVAILABLE:
            try:
                self.okt = Okt()
                self.kkma = Kkma()
                print("KoNLPy í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"KoNLPy ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                KONLPY_AVAILABLE = False
                self.okt = None
                self.kkma = None
        else:
            self.okt = None
            self.kkma = None
        
        # PyKoSpacing ì´ˆê¸°í™” (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        if PYKOSPACING_AVAILABLE:
            self.spacing = Spacing()
        else:
            self.spacing = None
        
        # ë¶ˆìš©ì–´ ì •ì˜
        self.stopwords = [
            'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
            'ê³¼', 'ì™€', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¿', 'ì•„ë‹ˆë¼', 'ì²˜ëŸ¼', 'ê°™ì´',
            'ê·¸', 'ì €', 'ì´', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ê²ƒ', 'ìˆ˜', 'ê³³', 'ë•Œ', 'ì ',
            'ì •ë§', 'ë„ˆë¬´', 'ì§„ì§œ', 'ì™„ì „', 'ë§¤ìš°', 'ì•„ì£¼', 'ì¢€', 'ì¡°ê¸ˆ'
        ]
    
    def load_data(self, file_path, text_column='review', rating_column='rating'):
        """
        ë¦¬ë·° ë°ì´í„° ë¡œë”©
        
        Args:
            file_path: ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            text_column: ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            rating_column: í‰ì  ì»¬ëŸ¼ëª…
        """
        self.df = pd.read_csv(file_path)
        self.text_col = text_column
        self.rating_col = rating_column
        
        print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(self.df)}ê°œ ë¦¬ë·°")
        print(f"ì»¬ëŸ¼: {list(self.df.columns)}")
        
        return self.df

    def preprocess_text(self, text):
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        - ë„ì–´ì“°ê¸° êµì • (PyKoSpacing ì‚¬ìš© ê°€ëŠ¥ì‹œ)
        - íŠ¹ìˆ˜ë¬¸ì ì œê±°
        - ì •ê·œí™”
        """
        if pd.isna(text):
            return ""
        
        # ë„ì–´ì“°ê¸° êµì • (PyKoSpacingì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
        if self.spacing is not None:
            try:
                text = self.spacing(text)
            except Exception as e:
                print(f"ë„ì–´ì“°ê¸° êµì • ì¤‘ ì˜¤ë¥˜: {e}")
                pass  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        else:
            # ê°„ë‹¨í•œ ë„ì–´ì“°ê¸° ê·œì¹™ ì ìš©
            text = self.simple_spacing_correction(text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€)
        text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', text)
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def simple_spacing_correction(self, text):
        """
        ê°„ë‹¨í•œ ë„ì–´ì“°ê¸° ê·œì¹™ ì ìš© (PyKoSpacing ëŒ€ì²´)
        """
        # ê¸°ë³¸ì ì¸ ë„ì–´ì“°ê¸° ê·œì¹™ë“¤
        spacing_rules = [
            # ì¡°ì‚¬ ì•ì— ë„ì–´ì“°ê¸°
            (r'([ê°€-í£])([ì€ëŠ”ì´ê°€ì„ë¥¼])([ê°€-í£])', r'\1 \2 \3'),
            # ì–´ë¯¸ ì•ì— ë„ì–´ì“°ê¸°  
            (r'([ê°€-í£])([ìŠµë‹ˆë‹¤í•´ìš”])([ê°€-í£])', r'\1\2 \3'),
            # ìˆ«ìì™€ ë‹¨ìœ„ ì‚¬ì´
            (r'([0-9])([ê°€-í£])', r'\1 \2'),
            # ì˜ì–´ì™€ í•œê¸€ ì‚¬ì´
            (r'([a-zA-Z])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([a-zA-Z])', r'\1 \2'),
        ]
        
        for pattern, replacement in spacing_rules:
            text = re.sub(pattern, replacement, text)
        
        return text
    
    def clean_reviews(self):
        """ì „ì²´ ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬"""
        print("ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
        
        self.df['cleaned_review'] = self.df[self.text_col].apply(self.preprocess_text)
        
        # ë¹ˆ ë¦¬ë·° ì œê±°
        self.df = self.df[self.df['cleaned_review'].str.len() > 0].reset_index(drop=True)
        
        print(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(self.df)}ê°œ ë¦¬ë·°")
        
        return self.df

    def tokenize_with_okt(self, text, pos_filter=['Noun', 'Verb', 'Adjective']):
        """OKTë¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„ (Java í•„ìš”)"""
        if not text:
            return []
        
        if not KONLPY_AVAILABLE or self.okt is None:
            print("KoNLPyë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Kiwië¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            return self.tokenize_with_kiwi(text)
        
        try:
            tokens = self.okt.pos(text, stem=True)
            filtered_tokens = [
                word for word, pos in tokens 
                if pos in pos_filter and word not in self.stopwords and len(word) > 1
            ]
            return filtered_tokens
        except Exception as e:
            print(f"OKT ë¶„ì„ ì˜¤ë¥˜: {e}")
            return self.tokenize_with_kiwi(text)
    
    def tokenize_with_kiwi(self, text, pos_filter=['NNG', 'NNP', 'VA', 'VV']):
        """Kiwië¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„"""
        if not text:
            return []
        
        tokens = self.kiwi.tokenize(text)
        filtered_tokens = [
            token.form for token in tokens
            if token.tag in pos_filter and token.form not in self.stopwords and len(token.form) > 1
        ]
        
        return filtered_tokens
    
    def extract_words_with_soynlp(self, min_count=10, max_length=10):
        """soynlpë¥¼ ì´ìš©í•œ ë‹¨ì–´ ì¶”ì¶œ"""
        print("soynlpë¡œ ë‹¨ì–´ ì¶”ì¶œ ì¤‘...")
        
        texts = self.df['cleaned_review'].tolist()
        
        word_extractor = WordExtractor(
            min_count=min_count,
            min_length=2,
            max_length=max_length
        )
        
        word_extractor.train(texts)
        words = word_extractor.extract()
        
        # ë‹¨ì–´ ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        word_scores = {
            word: score.cohesion_forward * score.right_branching_entropy
            for word, score in words.items()
        }
        
        return word_scores
    
    def tokenize_reviews(self, method='kiwi'):
        """ì „ì²´ ë¦¬ë·° í† í°í™”"""
        print(f"{method}ë¡œ ë¦¬ë·° í† í°í™” ì¤‘...")
        
        if method == 'okt':
            self.df['tokens'] = self.df['cleaned_review'].apply(self.tokenize_with_okt)
        elif method == 'kiwi':
            self.df['tokens'] = self.df['cleaned_review'].apply(self.tokenize_with_kiwi)
        
        # í† í°í™”ëœ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì¼ë¶€ ë¶„ì„ì—ì„œ í•„ìš”)
        self.df['tokens_str'] = self.df['tokens'].apply(lambda x: ' '.join(x))
        
        print("í† í°í™” ì™„ë£Œ")
        return self.df

    def load_sentiment_dict(self, dict_path=None):
        """í•œêµ­ì–´ ê°ì„±ì‚¬ì „ ë¡œë”©"""
        # í™•ì¥ëœ ê°ì„±ì‚¬ì „
        positive_words = [
            'ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'ì™„ë²½', 'ìš°ìˆ˜', 'íƒì›”',
            'ë›°ì–´ë‚˜ë‹¤', 'ë©‹ì§€ë‹¤', 'ì„±ê³µ', 'íš¨ê³¼', 'í’ˆì§ˆ', 'ê°€ì„±ë¹„', 'ê¹”ë”í•˜ë‹¤',
            'ì˜ˆì˜ë‹¤', 'ê´œì°®ë‹¤', 'ë§ˆìŒì—ë“¤ë‹¤', 'ë“ ë“ í•˜ë‹¤', 'í¸ë¦¬í•˜ë‹¤', 'ìœ ìš©í•˜ë‹¤',
            'ë¹ ë¥´ë‹¤', 'ì •í™•í•˜ë‹¤', 'ì¹œì ˆí•˜ë‹¤', 'ì‹¸ë‹¤', 'ì €ë ´í•˜ë‹¤', 'í•©ë¦¬ì ',
            'íš¨ìœ¨ì ', 'ì‹ ì†í•˜ë‹¤', 'ë§Œì¡±ìŠ¤ëŸ½ë‹¤', 'ê°ì‚¬í•˜ë‹¤', 'ê³ ë§™ë‹¤', 'ë„ì›€',
            'í¸í•˜ë‹¤', 'ì‰½ë‹¤', 'ê°„í¸í•˜ë‹¤', 'ì‹¤ìš©ì ', 'ê²½ì œì ', 'ì•Œëœ°í•˜ë‹¤'
        ]
        
        negative_words = [
            'ë‚˜ì˜ë‹¤', 'ë³„ë¡œ', 'ìµœì•…', 'ì‹¤ë§', 'ë¶ˆë§Œ', 'ë¬¸ì œ', 'ë¶€ì¡±', 'ì•„ì‰½ë‹¤',
            'í›„íšŒ', 'ë¹„ì¶”', 'ê°œì„ ', 'ë‹¨ì ', 'ë¶ˆí¸', 'ì˜¤ë¥˜', 'ëŠë¦¬ë‹¤', 'ë¹„ì‹¸ë‹¤',
            'ì–´ë µë‹¤', 'ë³µì¡í•˜ë‹¤', 'ê·€ì°®ë‹¤', 'ì§œì¦', 'í™”ë‚˜ë‹¤', 'ì†ìƒí•˜ë‹¤',
            'ê±±ì •', 'ë¶ˆì•ˆ', 'ê³ ì¥', 'íŒŒì†', 'ê²°í•¨', 'í ì§‘', 'ë”ëŸ½ë‹¤',
            'ëƒ„ìƒˆ', 'ì‹œë„ëŸ½ë‹¤', 'ë¬´ê²ë‹¤', 'ì‘ë‹¤', 'í¬ë‹¤', 'ë”±ë”±í•˜ë‹¤', 'ì§ˆê¸°ë‹¤'
        ]
        
        self.sentiment_dict = {
            'positive': positive_words,
            'negative': negative_words
        }
        
        return self.sentiment_dict
    
    def analyze_sentiment_rule_based(self, tokens):
        """ê°œì„ ëœ ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„"""
        if not hasattr(self, 'sentiment_dict'):
            self.load_sentiment_dict()
        
        pos_score = sum(1 for token in tokens if token in self.sentiment_dict['positive'])
        neg_score = sum(1 for token in tokens if token in self.sentiment_dict['negative'])
        
        # ì ìˆ˜ ì°¨ì´ë¡œ ì¤‘ë¦½ íŒë‹¨
        if abs(pos_score - neg_score) <= 1 and (pos_score + neg_score) <= 2:
            return 'neutral'
        elif pos_score > neg_score:
            return 'positive'
        elif neg_score > pos_score:
            return 'negative'
        else:
            return 'neutral'
    
    def analyze_sentiment_rating_based(self, rating, threshold=3):
        """í‰ì  ê¸°ë°˜ ê°ì„± ë¼ë²¨ë§"""
        if rating > threshold:
            return 'positive'
        elif rating < threshold:
            return 'negative'
        else:
            return 'neutral'
    
    def create_sentiment_labels(self, method='rating'):
        """ê°ì„± ë¼ë²¨ ìƒì„±"""
        print(f"{method} ë°©ì‹ìœ¼ë¡œ ê°ì„± ë¼ë²¨ ìƒì„± ì¤‘...")
        
        if method == 'rating':
            self.df['sentiment'] = self.df[self.rating_col].apply(self.analyze_sentiment_rating_based)
        elif method == 'rule':
            self.df['sentiment'] = self.df['tokens'].apply(self.analyze_sentiment_rule_based)
        
        print("ê°ì„± ë¼ë²¨ ìƒì„± ì™„ë£Œ")
        print(self.df['sentiment'].value_counts())
        
        return self.df

    def extract_keywords_tfidf(self, max_features=100, ngram_range=(1, 2)):
        """TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("TF-IDFë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=ngram_range,
            token_pattern=r'\b\w+\b'
        )
        
        tfidf_matrix = vectorizer.fit_transform(self.df['tokens_str'])
        feature_names = vectorizer.get_feature_names_out()
        
        # ë‹¨ì–´ë³„ TF-IDF ì ìˆ˜ ê³„ì‚°
        tfidf_scores = tfidf_matrix.sum(axis=0).A1
        keyword_scores = list(zip(feature_names, tfidf_scores))
        keyword_scores.sort(key=lambda x: x[1], reverse=True)
        
        self.tfidf_keywords = keyword_scores
        
        return keyword_scores
    
    def extract_keywords_krwordrank(self, min_count=5, max_length=10):
        """KR-WordRankë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        print("KR-WordRankë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        texts = self.df['cleaned_review'].tolist()
        
        wordrank_extractor = KRWordRank(
            min_count=min_count,
            max_length=max_length
        )
        
        beta = 0.85
        max_iter = 10
        
        keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)
        
        # í‚¤ì›Œë“œë¥¼ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)
        
        self.krwordrank_keywords = sorted_keywords
        
        return sorted_keywords

    def topic_modeling_lda(self, n_topics=5, max_features=100):
        """LDA í† í”½ ëª¨ë¸ë§"""
        print(f"LDAë¡œ {n_topics}ê°œ í† í”½ ì¶”ì¶œ ì¤‘...")
        
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            token_pattern=r'\b\w+\b'
        )
        
        tfidf_matrix = vectorizer.fit_transform(self.df['tokens_str'])
        
        lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=42
        )
        
        lda.fit(tfidf_matrix)
        
        # í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
        feature_names = vectorizer.get_feature_names_out()
        topics = []
        
        for topic_idx, topic in enumerate(lda.components_):
            top_words_idx = topic.argsort()[-10:][::-1]
            top_words = [feature_names[i] for i in top_words_idx]
            topics.append(top_words)
        
        self.topics = topics
        
        return topics

    def plot_sentiment_distribution(self):
        """ê°ì„± ë¶„í¬ ì‹œê°í™”"""
        plt.figure(figsize=(10, 6))
        
        # ê°ì„±ë³„ ë¶„í¬
        plt.subplot(1, 2, 1)
        sentiment_counts = self.df['sentiment'].value_counts()
        plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
        plt.title('ê°ì„± ë¶„í¬')
        
        # í‰ì ë³„ ë¶„í¬
        plt.subplot(1, 2, 2)
        plt.hist(self.df[self.rating_col], bins=5, edgecolor='black')
        plt.xlabel('í‰ì ')
        plt.ylabel('ë¦¬ë·° ìˆ˜')
        plt.title('í‰ì  ë¶„í¬')
        
        plt.tight_layout()
        plt.show()
    
    def create_wordcloud(self, sentiment=None, max_words=100):
        """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
        if sentiment:
            text_data = self.df[self.df['sentiment'] == sentiment]['tokens_str']
            title = f'{sentiment.title()} ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ'
        else:
            text_data = self.df['tokens_str']
            title = 'ì „ì²´ ë¦¬ë·° ì›Œë“œí´ë¼ìš°ë“œ'
        
        text = ' '.join(text_data)
        
        # macOSì—ì„œ í•œê¸€ í°íŠ¸ ê²½ë¡œ ì„¤ì •
        font_paths = [
            '/System/Library/Fonts/NanumGothic.ttc',  # macOS ê¸°ë³¸ ê²½ë¡œ
            '/Library/Fonts/NanumGothic.ttf',
            '/System/Library/Fonts/Arial Unicode MS.ttf',  # ëŒ€ì•ˆ í°íŠ¸
            '/System/Library/Fonts/AppleGothic.ttf'
        ]
        
        font_path = None
        for path in font_paths:
            try:
                import os
                if os.path.exists(path):
                    font_path = path
                    break
            except:
                continue
        
        wordcloud_params = {
            'width': 800,
            'height': 400,
            'background_color': 'white',
            'max_words': max_words
        }
        
        if font_path:
            wordcloud_params['font_path'] = font_path
        
        wordcloud = WordCloud(**wordcloud_params).generate(text)
        
        plt.figure(figsize=(12, 6))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(title, fontsize=16)
        plt.show()
    
    def plot_keyword_ranking(self, method='tfidf', top_n=20):
        """í‚¤ì›Œë“œ ë­í‚¹ ì‹œê°í™”"""
        if method == 'tfidf' and hasattr(self, 'tfidf_keywords'):
            keywords = self.tfidf_keywords[:top_n]
            title = 'TF-IDF í‚¤ì›Œë“œ ë­í‚¹'
        elif method == 'krwordrank' and hasattr(self, 'krwordrank_keywords'):
            keywords = self.krwordrank_keywords[:top_n]
            title = 'KR-WordRank í‚¤ì›Œë“œ ë­í‚¹'
        else:
            print(f"{method} í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        words = [item[0] for item in keywords]
        scores = [item[1] for item in keywords]
        
        plt.figure(figsize=(12, 8))
        plt.barh(range(len(words)), scores)
        plt.yticks(range(len(words)), words)
        plt.xlabel('ì ìˆ˜')
        plt.title(title)
        plt.gca().invert_yaxis()
        plt.tight_layout()
        plt.show()

    def generate_analysis_report(self):
        """ë¶„ì„ ê²°ê³¼ ì¢…í•© ë¦¬í¬íŠ¸"""
        print("="*50)
        print("ìƒí’ˆ ë¦¬ë·° ë¶„ì„ ë¦¬í¬íŠ¸")
        print("="*50)
        
        # ê¸°ë³¸ í†µê³„
        print(f"\n1. ê¸°ë³¸ í†µê³„")
        print(f"   - ì´ ë¦¬ë·° ìˆ˜: {len(self.df):,}ê°œ")
        print(f"   - í‰ê·  í‰ì : {self.df[self.rating_col].mean():.2f}")
        print(f"   - í‰ê·  ë¦¬ë·° ê¸¸ì´: {self.df['cleaned_review'].str.len().mean():.1f}ì")
        
        # ê°ì„± ë¶„ì„ ê²°ê³¼
        if 'sentiment' in self.df.columns:
            print(f"\n2. ê°ì„± ë¶„ì„ ê²°ê³¼")
            sentiment_counts = self.df['sentiment'].value_counts()
            for sentiment, count in sentiment_counts.items():
                ratio = count / len(self.df) * 100
                print(f"   - {sentiment}: {count:,}ê°œ ({ratio:.1f}%)")
        
        # ì£¼ìš” í‚¤ì›Œë“œ
        if hasattr(self, 'tfidf_keywords'):
            print(f"\n3. ì£¼ìš” í‚¤ì›Œë“œ (TF-IDF)")
            for i, (word, score) in enumerate(self.tfidf_keywords[:10], 1):
                print(f"   {i:2d}. {word} ({score:.3f})")
        
        # í† í”½ ëª¨ë¸ë§ ê²°ê³¼
        if hasattr(self, 'topics'):
            print(f"\n4. ì£¼ìš” í† í”½")
            for i, topic_words in enumerate(self.topics, 1):
                print(f"   í† í”½ {i}: {', '.join(topic_words[:5])}")
        
        print("="*50)

# =============================================================================
# ì—‘ì…€ ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜ë“¤
# =============================================================================

def analyze_coupang_reviews():
    """ì¿ íŒ¡ ë¦¬ë·° ë°ì´í„° ë¶„ì„ ì‹¤í–‰"""
    
    # 1. ë°ì´í„° ë¡œë”© ë° í™•ì¸
    file_path = '/Users/brich/Desktop/marketcrawler/output/coupang_reviews_20250701_180647.csv'
    
    try:
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv(file_path)
        return df
        
    except FileNotFoundError:
        print("âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def identify_columns(df):
    """ë°ì´í„° ì»¬ëŸ¼ ë¶„ì„ ë° ë¦¬ë·°/í‰ì  ì»¬ëŸ¼ ì‹ë³„"""
    
    text_columns = []
    rating_columns = []
    
    for col in df.columns:
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì¶”ì • (ê¸´ ë¬¸ìì—´, ë†’ì€ ë‹¤ì–‘ì„±)
        if df[col].dtype == 'object':
            avg_length = df[col].dropna().astype(str).str.len().mean()
            if avg_length > 10:  # í‰ê·  ê¸¸ì´ê°€ 10ì ì´ìƒ
                text_columns.append(col)
        
        # í‰ì  ì»¬ëŸ¼ ì¶”ì • (ìˆ«ìí˜•, 1-5 ë˜ëŠ” 1-10 ë²”ìœ„)
        if df[col].dtype in ['int64', 'float64']:
            min_val = df[col].min()
            max_val = df[col].max()
            if 1 <= min_val and max_val <= 10:  # 1-10 ë²”ìœ„ì˜ ìˆ«ì
                rating_columns.append(col)
    
    return text_columns, rating_columns

def run_analysis_with_identified_columns(df, text_col, rating_col=None):
    """ì‹ë³„ëœ ì»¬ëŸ¼ìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰ ë° ì—‘ì…€ ê²°ê³¼ ìƒì„±"""
    
    analyzer = ReviewAnalyzer()
    
    # ë°ì´í„° ì„¤ì •
    analyzer.df = df.copy()
    analyzer.text_col = text_col
    analyzer.rating_col = rating_col
    
    # ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    analyzer.df['cleaned_review'] = analyzer.df[text_col].apply(analyzer.preprocess_text)
    
    # ë¹ˆ ë¦¬ë·° ì œê±°
    initial_count = len(analyzer.df)
    analyzer.df = analyzer.df[analyzer.df['cleaned_review'].str.len() > 0].reset_index(drop=True)
    final_count = len(analyzer.df)
    
    # 2. í˜•íƒœì†Œ ë¶„ì„
    analyzer.df['tokens'] = analyzer.df['cleaned_review'].apply(analyzer.tokenize_with_kiwi)
    analyzer.df['tokens_str'] = analyzer.df['tokens'].apply(lambda x: ' '.join(x))
    
    # 3. ê°ì„± ë¶„ì„
    analyzer.df['sentiment_rule'] = analyzer.df['tokens'].apply(analyzer.analyze_sentiment_rule_based)
    
    if rating_col and rating_col in df.columns:
        analyzer.df['sentiment_rating'] = analyzer.df[rating_col].apply(analyzer.analyze_sentiment_rating_based)
    
    # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords_tfidf = analyzer.extract_keywords_tfidf(max_features=50)
    
    try:
        keywords_krwordrank = analyzer.extract_keywords_krwordrank(min_count=3)
    except Exception as e:
        keywords_krwordrank = None
    
    # 5. í† í”½ ëª¨ë¸ë§
    try:
        topics = analyzer.topic_modeling_lda(n_topics=5, max_features=50)
    except Exception as e:
        topics = None
    
    # ì—‘ì…€ ê²°ê³¼ ìƒì„±
    create_excel_report(analyzer, keywords_tfidf, keywords_krwordrank, topics, 
                       initial_count, final_count, text_col, rating_col)
    
    return analyzer

def create_excel_report(analyzer, keywords_tfidf, keywords_krwordrank, topics, 
                       initial_count, final_count, text_col, rating_col):
    """ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì—‘ì…€ ì‹œíŠ¸ë¡œ ì €ì¥"""
    
    # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = '/Users/brich/Desktop/marketcrawler/output'
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_file = f"{output_dir}/coupang_analysis_report_{timestamp}.xlsx"
    
    # ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±
    all_data = []
    
    # 1. ë¶„ì„ ê°œìš”
    all_data.append(['=== ì¿ íŒ¡ ë¦¬ë·° ë¶„ì„ ë¦¬í¬íŠ¸ ===', '', ''])
    all_data.append(['ë¶„ì„ ì¼ì‹œ', datetime.now().strftime("%Y-%m-%d %H:%M:%S"), ''])
    all_data.append(['', '', ''])
    
    # 2. ê¸°ë³¸ í†µê³„
    all_data.append(['[ê¸°ë³¸ í†µê³„]', '', ''])
    all_data.append(['ì´ ë¦¬ë·° ìˆ˜ (ì›ë³¸)', f"{initial_count:,}ê°œ", 'ì²˜ë¦¬ ì „ ì „ì²´ ë¦¬ë·° ê°œìˆ˜'])
    all_data.append(['ë¶„ì„ëœ ë¦¬ë·° ìˆ˜', f"{final_count:,}ê°œ", 'ì „ì²˜ë¦¬ í›„ ì‹¤ì œ ë¶„ì„ëœ ë¦¬ë·° ê°œìˆ˜'])
    all_data.append(['ì œê±°ëœ ë¦¬ë·° ìˆ˜', f"{initial_count - final_count:,}ê°œ", 'ë¹ˆ ë‚´ìš© ë“±ìœ¼ë¡œ ì œê±°ëœ ë¦¬ë·°'])
    all_data.append(['ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼', text_col, 'ë¶„ì„ì— ì‚¬ìš©ëœ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…'])
    all_data.append(['í‰ì  ì»¬ëŸ¼', rating_col if rating_col else 'ì—†ìŒ', 'ë¶„ì„ì— ì‚¬ìš©ëœ í‰ì  ì»¬ëŸ¼ëª…'])
    
    # í…ìŠ¤íŠ¸ í†µê³„
    avg_length = analyzer.df['cleaned_review'].str.len().mean()
    max_length = analyzer.df['cleaned_review'].str.len().max()
    min_length = analyzer.df['cleaned_review'].str.len().min()
    
    all_data.append(['í‰ê·  ë¦¬ë·° ê¸¸ì´', f"{avg_length:.1f}ì", 'ì „ì²˜ë¦¬ëœ ë¦¬ë·°ì˜ í‰ê·  ê¸€ì ìˆ˜'])
    all_data.append(['ìµœëŒ€ ë¦¬ë·° ê¸¸ì´', f"{max_length}ì", 'ê°€ì¥ ê¸´ ë¦¬ë·°ì˜ ê¸€ì ìˆ˜'])
    all_data.append(['ìµœì†Œ ë¦¬ë·° ê¸¸ì´', f"{min_length}ì", 'ê°€ì¥ ì§§ì€ ë¦¬ë·°ì˜ ê¸€ì ìˆ˜'])
    
    # í‰ì  í†µê³„
    if rating_col and rating_col in analyzer.df.columns:
        avg_rating = analyzer.df[rating_col].mean()
        all_data.append(['í‰ê·  í‰ì ', f"{avg_rating:.2f}", 'ì „ì²´ ë¦¬ë·°ì˜ í‰ê·  í‰ì '])
        
        # í‰ì ë³„ ë¶„í¬
        rating_counts = analyzer.df[rating_col].value_counts().sort_index()
        for rating, count in rating_counts.items():
            ratio = count / len(analyzer.df) * 100
            all_data.append([f"{rating}ì  ë¦¬ë·° ìˆ˜", f"{count}ê°œ ({ratio:.1f}%)", 'í‰ì ë³„ ë¦¬ë·° ë¶„í¬'])
    
    all_data.append(['', '', ''])
    
    # 3. ê°ì„± ë¶„ì„ ê²°ê³¼
    all_data.append(['[ê°ì„± ë¶„ì„ ê²°ê³¼]', '', ''])
    sentiment_counts = analyzer.df['sentiment_rule'].value_counts()
    
    for sentiment in ['positive', 'negative', 'neutral']:
        count = sentiment_counts.get(sentiment, 0)
        ratio = count / len(analyzer.df) * 100
        sentiment_kr = {'positive': 'ê¸ì •', 'negative': 'ë¶€ì •', 'neutral': 'ì¤‘ë¦½'}[sentiment]
        all_data.append([f"{sentiment_kr} ë¦¬ë·°", f"{count:,}ê°œ ({ratio:.1f}%)", 'ê·œì¹™ ê¸°ë°˜ ê°ì„±ë¶„ì„ ê²°ê³¼'])
    
    # í‰ì  ê¸°ë°˜ ê°ì„± ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
    if 'sentiment_rating' in analyzer.df.columns:
        all_data.append(['', '', ''])
        all_data.append(['[í‰ì  ê¸°ë°˜ ê°ì„± ë¶„ì„]', '', ''])
        sentiment_rating_counts = analyzer.df['sentiment_rating'].value_counts()
        
        for sentiment in ['positive', 'negative', 'neutral']:
            count = sentiment_rating_counts.get(sentiment, 0)
            ratio = count / len(analyzer.df) * 100
            sentiment_kr = {'positive': 'ê¸ì •', 'negative': 'ë¶€ì •', 'neutral': 'ì¤‘ë¦½'}[sentiment]
            all_data.append([f"{sentiment_kr} ë¦¬ë·°", f"{count:,}ê°œ ({ratio:.1f}%)", 'í‰ì  ê¸°ë°˜ ê°ì„±ë¶„ì„ ê²°ê³¼'])
    
    all_data.append(['', '', ''])
    
    # 4. TF-IDF í‚¤ì›Œë“œ ë¶„ì„
    all_data.append(['[TF-IDF í‚¤ì›Œë“œ ë¶„ì„ TOP 20]', '', ''])
    all_data.append(['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'TF-IDF ì ìˆ˜'])
    
    for i, (word, score) in enumerate(keywords_tfidf[:20], 1):
        all_data.append([i, word, round(score, 4)])
    
    all_data.append(['', '', ''])
    
    # 5. KR-WordRank í‚¤ì›Œë“œ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
    if keywords_krwordrank:
        all_data.append(['[KR-WordRank í‚¤ì›Œë“œ ë¶„ì„ TOP 20]', '', ''])
        all_data.append(['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'WordRank ì ìˆ˜'])
        
        for i, (word, score) in enumerate(keywords_krwordrank[:20], 1):
            all_data.append([i, word, round(score, 1)])
        
        all_data.append(['', '', ''])
    
    # 6. í† í”½ ëª¨ë¸ë§ ê²°ê³¼
    if topics:
        all_data.append(['[í† í”½ ëª¨ë¸ë§ ê²°ê³¼]', '', ''])
        all_data.append(['í† í”½ ë²ˆí˜¸', 'ì£¼ìš” í‚¤ì›Œë“œ', 'ì„¤ëª…'])
        
        for i, topic_words in enumerate(topics, 1):
            keywords = ', '.join(topic_words[:8])
            all_data.append([f"í† í”½ {i}", keywords, f"í† í”½ {i}ì˜ ì£¼ìš” í‚¤ì›Œë“œë“¤"])
        
        all_data.append(['', '', ''])
    
    # 7. ê°ì„±ë³„ í‚¤ì›Œë“œ ë¶„ì„
    all_data.append(['[ê°ì„±ë³„ ì£¼ìš” í‚¤ì›Œë“œ]', '', ''])
    
    for sentiment in ['positive', 'negative', 'neutral']:
        if sentiment in analyzer.df['sentiment_rule'].values:
            sentiment_data = analyzer.df[analyzer.df['sentiment_rule'] == sentiment]
            if len(sentiment_data) > 0:
                sentiment_text = ' '.join(sentiment_data['tokens_str'])
                vectorizer = TfidfVectorizer(max_features=10, token_pattern=r'\b\w+\b')
                try:
                    tfidf_matrix = vectorizer.fit_transform([sentiment_text])
                    feature_names = vectorizer.get_feature_names_out()
                    tfidf_scores = tfidf_matrix.toarray()[0]
                    sentiment_keywords = sorted(zip(feature_names, tfidf_scores), 
                                              key=lambda x: x[1], reverse=True)
                    
                    sentiment_kr = {'positive': 'ê¸ì •', 'negative': 'ë¶€ì •', 'neutral': 'ì¤‘ë¦½'}[sentiment]
                    keywords_str = ', '.join([word for word, score in sentiment_keywords[:8]])
                    all_data.append([f"{sentiment_kr} í‚¤ì›Œë“œ", keywords_str, f"{sentiment_kr} ë¦¬ë·°ì˜ íŠ¹ì§•ì  í‚¤ì›Œë“œ"])
                except:
                    sentiment_kr = {'positive': 'ê¸ì •', 'negative': 'ë¶€ì •', 'neutral': 'ì¤‘ë¦½'}[sentiment]
                    all_data.append([f"{sentiment_kr} í‚¤ì›Œë“œ", 'í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨', ''])
    
    all_data.append(['', '', ''])
    
    # 8. ìƒì„¸ í†µê³„
    all_data.append(['[ìƒì„¸ í†µê³„]', '', ''])
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
    length_stats = analyzer.df['cleaned_review'].str.len().describe()
    all_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´ í‰ê· ', f"{length_stats['mean']:.1f}ì", ''])
    all_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´ í‘œì¤€í¸ì°¨', f"{length_stats['std']:.1f}ì", ''])
    all_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì†Ÿê°’', f"{length_stats['min']:.0f}ì", ''])
    all_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´ ì¤‘ê°„ê°’', f"{length_stats['50%']:.0f}ì", ''])
    all_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœëŒ“ê°’', f"{length_stats['max']:.0f}ì", ''])
    
    # í‚¤ì›Œë“œ ìˆ˜ í†µê³„
    token_counts = analyzer.df['tokens'].apply(len)
    token_stats = token_counts.describe()
    all_data.append(['í‚¤ì›Œë“œ ìˆ˜ í‰ê· ', f"{token_stats['mean']:.1f}ê°œ", ''])
    all_data.append(['í‚¤ì›Œë“œ ìˆ˜ ì¤‘ê°„ê°’', f"{token_stats['50%']:.0f}ê°œ", ''])
    all_data.append(['í‚¤ì›Œë“œ ìˆ˜ ìµœëŒ“ê°’', f"{token_stats['max']:.0f}ê°œ", ''])
    
    all_data.append(['', '', ''])
    
    # 9. ìƒ˜í”Œ ë¦¬ë·° ë°ì´í„° (ìƒìœ„ 10ê°œ)
    all_data.append(['[ìƒ˜í”Œ ë¦¬ë·° ë°ì´í„° (ìƒìœ„ 10ê°œ)]', '', ''])
    all_data.append(['ì›ë³¸ ë¦¬ë·°', 'ê·œì¹™ê¸°ë°˜ ê°ì„±', 'ì¶”ì¶œëœ í‚¤ì›Œë“œ'])
    
    sample_df = analyzer.df.head(10)
    for idx, row in sample_df.iterrows():
        original_review = str(row[text_col])[:100] + "..." if len(str(row[text_col])) > 100 else str(row[text_col])
        sentiment = row['sentiment_rule']
        keywords = row['tokens_str'][:50] + "..." if len(row['tokens_str']) > 50 else row['tokens_str']
        all_data.append([original_review, sentiment, keywords])
    
    # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì—‘ì…€ ì €ì¥
    df_report = pd.DataFrame(all_data, columns=['í•­ëª©', 'ê°’', 'ì„¤ëª…/ì¶”ê°€ì •ë³´'])
    
    # ExcelWriterë¡œ ì €ì¥
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        df_report.to_excel(writer, sheet_name='ì¿ íŒ¡_ë¦¬ë·°_ë¶„ì„_ë¦¬í¬íŠ¸', index=False)
        
        # ì›Œí¬ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
        worksheet = writer.sheets['ì¿ íŒ¡_ë¦¬ë·°_ë¶„ì„_ë¦¬í¬íŠ¸']
        
        # ì»¬ëŸ¼ í­ ì¡°ì •
        worksheet.column_dimensions['A'].width = 30
        worksheet.column_dimensions['B'].width = 40
        worksheet.column_dimensions['C'].width = 60
        
        # í—¤ë” ìŠ¤íƒ€ì¼ë§
        from openpyxl.styles import Font, PatternFill, Alignment
        
        header_font = Font(bold=True, size=12)
        header_fill = PatternFill(start_color="CCCCCC", end_color="CCCCCC", fill_type="solid")
        
        for col in range(1, 4):
            cell = worksheet.cell(row=1, column=col)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal="center")
        
        # ì„¹ì…˜ í—¤ë” ìŠ¤íƒ€ì¼ë§ (ëŒ€ê´„í˜¸ë¡œ ì‹œì‘í•˜ëŠ” í–‰ë“¤)
        section_font = Font(bold=True, size=11, color="0066CC")
        section_fill = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
        
        for row in range(2, len(all_data) + 2):
            cell_value = worksheet.cell(row=row, column=1).value
            if cell_value and str(cell_value).startswith('[') and str(cell_value).endswith(']'):
                for col in range(1, 4):
                    cell = worksheet.cell(row=row, column=col)
                    cell.font = section_font
                    cell.fill = section_fill
    
    print(f"âœ… ë¶„ì„ ê²°ê³¼ê°€ í•˜ë‚˜ì˜ ì—‘ì…€ ì‹œíŠ¸ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
    print(f"ğŸ“ íŒŒì¼ ê²½ë¡œ: {output_file}")
    
    return output_file

def create_summary_sheet(writer, analyzer, initial_count, final_count, text_col, rating_col):
    """ìš”ì•½ ì •ë³´ ì‹œíŠ¸ ìƒì„±"""
    
    summary_data = []
    
    # ê¸°ë³¸ ì •ë³´
    summary_data.append(['ë¶„ì„ í•­ëª©', 'ê°’', 'ì„¤ëª…'])
    summary_data.append(['ë¶„ì„ ì¼ì‹œ', datetime.now().strftime("%Y-%m-%d %H:%M:%S"), 'ë¶„ì„ì„ ìˆ˜í–‰í•œ ë‚ ì§œì™€ ì‹œê°„'])
    summary_data.append(['ì´ ë¦¬ë·° ìˆ˜ (ì›ë³¸)', f"{initial_count:,}ê°œ", 'ì²˜ë¦¬ ì „ ì „ì²´ ë¦¬ë·° ê°œìˆ˜'])
    summary_data.append(['ë¶„ì„ëœ ë¦¬ë·° ìˆ˜', f"{final_count:,}ê°œ", 'ì „ì²˜ë¦¬ í›„ ì‹¤ì œ ë¶„ì„ëœ ë¦¬ë·° ê°œìˆ˜'])
    summary_data.append(['ì œê±°ëœ ë¦¬ë·° ìˆ˜', f"{initial_count - final_count:,}ê°œ", 'ë¹ˆ ë‚´ìš© ë“±ìœ¼ë¡œ ì œê±°ëœ ë¦¬ë·°'])
    summary_data.append(['ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼', text_col, 'ë¶„ì„ì— ì‚¬ìš©ëœ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…'])
    summary_data.append(['í‰ì  ì»¬ëŸ¼', rating_col if rating_col else 'ì—†ìŒ', 'ë¶„ì„ì— ì‚¬ìš©ëœ í‰ì  ì»¬ëŸ¼ëª…'])
    
    # í…ìŠ¤íŠ¸ í†µê³„
    avg_length = analyzer.df['cleaned_review'].str.len().mean()
    max_length = analyzer.df['cleaned_review'].str.len().max()
    min_length = analyzer.df['cleaned_review'].str.len().min()
    
    summary_data.append(['í‰ê·  ë¦¬ë·° ê¸¸ì´', f"{avg_length:.1f}ì", 'ì „ì²˜ë¦¬ëœ ë¦¬ë·°ì˜ í‰ê·  ê¸€ì ìˆ˜'])
    summary_data.append(['ìµœëŒ€ ë¦¬ë·° ê¸¸ì´', f"{max_length}ì", 'ê°€ì¥ ê¸´ ë¦¬ë·°ì˜ ê¸€ì ìˆ˜'])
    summary_data.append(['ìµœì†Œ ë¦¬ë·° ê¸¸ì´', f"{min_length}ì", 'ê°€ì¥ ì§§ì€ ë¦¬ë·°ì˜ ê¸€ì ìˆ˜'])
    
    # í‰ì  í†µê³„
    if rating_col and rating_col in analyzer.df.columns:
        avg_rating = analyzer.df[rating_col].mean()
        summary_data.append(['í‰ê·  í‰ì ', f"{avg_rating:.2f}", 'ì „ì²´ ë¦¬ë·°ì˜ í‰ê·  í‰ì '])
    
    # ê°ì„± ë¶„í¬
    sentiment_counts = analyzer.df['sentiment_rule'].value_counts()
    summary_data.append(['ê¸ì • ë¦¬ë·° ìˆ˜', f"{sentiment_counts.get('positive', 0):,}ê°œ", 'ê·œì¹™ ê¸°ë°˜ ê°ì„±ë¶„ì„ ê²°ê³¼'])
    summary_data.append(['ë¶€ì • ë¦¬ë·° ìˆ˜', f"{sentiment_counts.get('negative', 0):,}ê°œ", 'ê·œì¹™ ê¸°ë°˜ ê°ì„±ë¶„ì„ ê²°ê³¼'])
    summary_data.append(['ì¤‘ë¦½ ë¦¬ë·° ìˆ˜', f"{sentiment_counts.get('neutral', 0):,}ê°œ", 'ê·œì¹™ ê¸°ë°˜ ê°ì„±ë¶„ì„ ê²°ê³¼'])
    
    summary_df = pd.DataFrame(summary_data[1:], columns=summary_data[0])
    summary_df.to_excel(writer, sheet_name='1_ë¶„ì„ìš”ì•½', index=False)

def create_sentiment_analysis_sheet(writer, analyzer):
    """ê°ì„± ë¶„ì„ ê²°ê³¼ ì‹œíŠ¸ ìƒì„±"""
    
    # ê·œì¹™ ê¸°ë°˜ ê°ì„± ë¶„ì„ ê²°ê³¼
    sentiment_rule_counts = analyzer.df['sentiment_rule'].value_counts()
    sentiment_rule_ratio = (sentiment_rule_counts / len(analyzer.df) * 100).round(1)
    
    sentiment_data = []
    sentiment_data.append(['ê°ì„±', 'ê°œìˆ˜', 'ë¹„ìœ¨(%)', 'ë¶„ì„ë°©ë²•'])
    
    for sentiment in ['positive', 'negative', 'neutral']:
        count = sentiment_rule_counts.get(sentiment, 0)
        ratio = sentiment_rule_ratio.get(sentiment, 0)
        sentiment_data.append([sentiment, count, ratio, 'ê·œì¹™ê¸°ë°˜'])
    
    # í‰ì  ê¸°ë°˜ ê°ì„± ë¶„ì„ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
    if 'sentiment_rating' in analyzer.df.columns:
        sentiment_rating_counts = analyzer.df['sentiment_rating'].value_counts()
        sentiment_rating_ratio = (sentiment_rating_counts / len(analyzer.df) * 100).round(1)
        
        for sentiment in ['positive', 'negative', 'neutral']:
            count = sentiment_rating_counts.get(sentiment, 0)
            ratio = sentiment_rating_ratio.get(sentiment, 0)
            sentiment_data.append([sentiment, count, ratio, 'í‰ì ê¸°ë°˜'])
    
    sentiment_df = pd.DataFrame(sentiment_data[1:], columns=sentiment_data[0])
    sentiment_df.to_excel(writer, sheet_name='2_ê°ì„±ë¶„ì„ê²°ê³¼', index=False)

def create_keyword_analysis_sheet(writer, keywords_tfidf, keywords_krwordrank):
    """í‚¤ì›Œë“œ ë¶„ì„ ì‹œíŠ¸ ìƒì„±"""
    
    # TF-IDF í‚¤ì›Œë“œ
    tfidf_data = []
    tfidf_data.append(['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'TF-IDF ì ìˆ˜'])
    
    for i, (word, score) in enumerate(keywords_tfidf[:30], 1):
        tfidf_data.append([i, word, round(score, 4)])
    
    tfidf_df = pd.DataFrame(tfidf_data[1:], columns=tfidf_data[0])
    
    # KR-WordRank í‚¤ì›Œë“œ (ìˆëŠ” ê²½ìš°)
    if keywords_krwordrank:
        krwordrank_data = []
        krwordrank_data.append(['ìˆœìœ„', 'í‚¤ì›Œë“œ', 'WordRank ì ìˆ˜'])
        
        for i, (word, score) in enumerate(keywords_krwordrank[:30], 1):
            krwordrank_data.append([i, word, round(score, 1)])
        
        krwordrank_df = pd.DataFrame(krwordrank_data[1:], columns=krwordrank_data[0])
        
        # ë‘ ê²°ê³¼ë¥¼ ë‚˜ë€íˆ ë°°ì¹˜
        combined_df = pd.concat([tfidf_df, pd.DataFrame([''] * len(tfidf_df)), krwordrank_df], axis=1)
        combined_df.columns = ['TF-IDF ìˆœìœ„', 'TF-IDF í‚¤ì›Œë“œ', 'TF-IDF ì ìˆ˜', '', 
                              'WordRank ìˆœìœ„', 'WordRank í‚¤ì›Œë“œ', 'WordRank ì ìˆ˜']
    else:
        combined_df = tfidf_df
    
    combined_df.to_excel(writer, sheet_name='3_í‚¤ì›Œë“œë¶„ì„', index=False)

def create_topic_modeling_sheet(writer, topics):
    """í† í”½ ëª¨ë¸ë§ ì‹œíŠ¸ ìƒì„±"""
    
    topic_data = []
    topic_data.append(['í† í”½ ë²ˆí˜¸', 'ì£¼ìš” í‚¤ì›Œë“œ', 'í‚¤ì›Œë“œ ì„¤ëª…'])
    
    for i, topic_words in enumerate(topics, 1):
        keywords = ', '.join(topic_words[:10])
        description = f"í† í”½ {i}ì˜ ì£¼ìš” í‚¤ì›Œë“œë“¤"
        topic_data.append([f"í† í”½ {i}", keywords, description])
    
    topic_df = pd.DataFrame(topic_data[1:], columns=topic_data[0])
    topic_df.to_excel(writer, sheet_name='4_í† í”½ëª¨ë¸ë§', index=False)

def create_sentiment_keywords_sheet(writer, analyzer):
    """ê°ì„±ë³„ í‚¤ì›Œë“œ ì‹œíŠ¸ ìƒì„±"""
    
    sentiment_keywords_data = []
    sentiment_keywords_data.append(['ê°ì„±', 'ìˆœìœ„', 'í‚¤ì›Œë“œ', 'TF-IDF ì ìˆ˜'])
    
    for sentiment in ['positive', 'negative', 'neutral']:
        if sentiment in analyzer.df['sentiment_rule'].values:
            sentiment_data = analyzer.df[analyzer.df['sentiment_rule'] == sentiment]
            if len(sentiment_data) > 0:
                sentiment_text = ' '.join(sentiment_data['tokens_str'])
                vectorizer = TfidfVectorizer(max_features=20, token_pattern=r'\b\w+\b')
                try:
                    tfidf_matrix = vectorizer.fit_transform([sentiment_text])
                    feature_names = vectorizer.get_feature_names_out()
                    tfidf_scores = tfidf_matrix.toarray()[0]
                    sentiment_keywords = sorted(zip(feature_names, tfidf_scores), 
                                              key=lambda x: x[1], reverse=True)
                    
                    for i, (word, score) in enumerate(sentiment_keywords, 1):
                        sentiment_keywords_data.append([sentiment, i, word, round(score, 4)])
                except:
                    sentiment_keywords_data.append([sentiment, 1, 'í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨', 0])
    
    sentiment_keywords_df = pd.DataFrame(sentiment_keywords_data[1:], columns=sentiment_keywords_data[0])
    sentiment_keywords_df.to_excel(writer, sheet_name='5_ê°ì„±ë³„í‚¤ì›Œë“œ', index=False)

def create_detailed_reviews_sheet(writer, analyzer):
    """ìƒì„¸ ë¦¬ë·° ë°ì´í„° ì‹œíŠ¸ ìƒì„±"""
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
    columns_to_include = [analyzer.text_col, 'cleaned_review', 'sentiment_rule']
    
    if analyzer.rating_col and analyzer.rating_col in analyzer.df.columns:
        columns_to_include.append(analyzer.rating_col)
        columns_to_include.append('sentiment_rating')
    
    columns_to_include.extend(['tokens_str'])
    
    detailed_df = analyzer.df[columns_to_include].copy()
    
    # ì»¬ëŸ¼ëª… ë³€ê²½
    column_names = {
        analyzer.text_col: 'ì›ë³¸_ë¦¬ë·°',
        'cleaned_review': 'ì „ì²˜ë¦¬ëœ_ë¦¬ë·°',
        'sentiment_rule': 'ê·œì¹™ê¸°ë°˜_ê°ì„±',
        'tokens_str': 'ì¶”ì¶œëœ_í‚¤ì›Œë“œ'
    }
    
    if analyzer.rating_col and analyzer.rating_col in analyzer.df.columns:
        column_names[analyzer.rating_col] = 'í‰ì '
        column_names['sentiment_rating'] = 'í‰ì ê¸°ë°˜_ê°ì„±'
    
    detailed_df = detailed_df.rename(columns=column_names)
    
    # ì—‘ì…€ ì‹œíŠ¸ í¬ê¸° ì œí•œì„ ê³ ë ¤í•˜ì—¬ ìµœëŒ€ 10000ê°œê¹Œì§€ë§Œ
    if len(detailed_df) > 10000:
        detailed_df = detailed_df.head(10000)
    
    detailed_df.to_excel(writer, sheet_name='6_ìƒì„¸ë¦¬ë·°ë°ì´í„°', index=False)

def create_statistics_sheet(writer, analyzer, rating_col):
    """í†µê³„ ìš”ì•½ ì‹œíŠ¸ ìƒì„±"""
    
    stats_data = []
    stats_data.append(['êµ¬ë¶„', 'í•­ëª©', 'ê°’'])
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ í†µê³„
    length_stats = analyzer.df['cleaned_review'].str.len().describe()
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', 'í‰ê· ', f"{length_stats['mean']:.1f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', 'í‘œì¤€í¸ì°¨', f"{length_stats['std']:.1f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', 'ìµœì†Ÿê°’', f"{length_stats['min']:.0f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', '25% ë¶„ìœ„ìˆ˜', f"{length_stats['25%']:.0f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', 'ì¤‘ê°„ê°’', f"{length_stats['50%']:.0f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', '75% ë¶„ìœ„ìˆ˜', f"{length_stats['75%']:.0f}ì"])
    stats_data.append(['í…ìŠ¤íŠ¸ ê¸¸ì´', 'ìµœëŒ“ê°’', f"{length_stats['max']:.0f}ì"])
    
    # í‰ì  í†µê³„ (ìˆëŠ” ê²½ìš°)
    if rating_col and rating_col in analyzer.df.columns:
        rating_stats = analyzer.df[rating_col].describe()
        stats_data.append(['í‰ì ', 'í‰ê· ', f"{rating_stats['mean']:.2f}"])
        stats_data.append(['í‰ì ', 'í‘œì¤€í¸ì°¨', f"{rating_stats['std']:.2f}"])
        stats_data.append(['í‰ì ', 'ìµœì†Ÿê°’', f"{rating_stats['min']:.0f}"])
        stats_data.append(['í‰ì ', 'ì¤‘ê°„ê°’', f"{rating_stats['50%']:.0f}"])
        stats_data.append(['í‰ì ', 'ìµœëŒ“ê°’', f"{rating_stats['max']:.0f}"])
        
        # í‰ì ë³„ ë¶„í¬
        rating_counts = analyzer.df[rating_col].value_counts().sort_index()
        for rating, count in rating_counts.items():
            ratio = count / len(analyzer.df) * 100
            stats_data.append(['í‰ì  ë¶„í¬', f"{rating}ì ", f"{count}ê°œ ({ratio:.1f}%)"])
    
    # í† í° ìˆ˜ í†µê³„
    token_counts = analyzer.df['tokens'].apply(len)
    token_stats = token_counts.describe()
    stats_data.append(['í‚¤ì›Œë“œ ìˆ˜', 'í‰ê· ', f"{token_stats['mean']:.1f}ê°œ"])
    stats_data.append(['í‚¤ì›Œë“œ ìˆ˜', 'ì¤‘ê°„ê°’', f"{token_stats['50%']:.0f}ê°œ"])
    stats_data.append(['í‚¤ì›Œë“œ ìˆ˜', 'ìµœëŒ“ê°’', f"{token_stats['max']:.0f}ê°œ"])
    
    stats_df = pd.DataFrame(stats_data[1:], columns=stats_data[0])
    stats_df.to_excel(writer, sheet_name='7_í†µê³„ìš”ì•½', index=False)

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ ì¿ íŒ¡ ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 50)
    
    # 1. ë°ì´í„° ë¡œë”©
    df = analyze_coupang_reviews()
    if df is None:
        return
    
    print(f"âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ: {len(df):,}ê°œ ë¦¬ë·°")
    print(f"ğŸ“Š ì»¬ëŸ¼ ì •ë³´: {list(df.columns)}")
    
    # 2. ì»¬ëŸ¼ ë¶„ì„
    text_columns, rating_columns = identify_columns(df)
    
    print(f"ğŸ” í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í›„ë³´: {text_columns}")
    print(f"ğŸ” í‰ì  ì»¬ëŸ¼ í›„ë³´: {rating_columns}")
    
    # 3. ì»¬ëŸ¼ ìë™ ì„ íƒ
    if text_columns:
        selected_text_col = text_columns[0]  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ
        print(f"ğŸ“ ì„ íƒëœ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼: '{selected_text_col}'")
    else:
        print("âŒ ë¦¬ë·° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    if rating_columns:
        selected_rating_col = rating_columns[0]  # ì²« ë²ˆì§¸ í‰ì  ì»¬ëŸ¼ ì„ íƒ
        print(f"â­ ì„ íƒëœ í‰ì  ì»¬ëŸ¼: '{selected_rating_col}'")
    else:
        selected_rating_col = None
        print("âš ï¸ í‰ì  ì»¬ëŸ¼ ì—†ìŒ (í…ìŠ¤íŠ¸ ê¸°ë°˜ ë¶„ì„ë§Œ ìˆ˜í–‰)")
    
    print("\nğŸ”„ ë¶„ì„ ì‹œì‘...")
    print("=" * 50)
    
    # 4. ë¶„ì„ ì‹¤í–‰ ë° ì—‘ì…€ ì¶œë ¥
    analyzer = run_analysis_with_identified_columns(df, selected_text_col, selected_rating_col)
    
    print("=" * 50)
    print("ğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()