"""
gs_crawler_human_dbg.py â€“ GS SHOP ì¸í”¼ë‹ˆí‹°ìŠ¤í¬ë¡¤ í¬ë¡¤ëŸ¬
  â€¢ ì‚¬ëŒì²˜ëŸ¼ 400~800px ëœë¤ ìŠ¤í¬ë¡¤
  â€¢ ìŠ¤í”¼ë„ˆ ê°ì§€ + ìƒí’ˆ ì¦ê°€ ëª¨ë‹ˆí„°
  â€¢ í’ë¶€í•œ DEBUG ë¡œê·¸ & ìŠ¤í¬ë¦°ìƒ·
"""
from __future__ import annotations
import ssl; ssl._create_default_https_context = ssl._create_unverified_context

import argparse, random, time, re, datetime, sys, pytz, logging, math, uuid
from pathlib import Path
from typing import List, Dict, Any

import pandas as pd
from bs4 import BeautifulSoup

import undetected_chromedriver as uc
from selenium_stealth import stealth
from fake_useragent import UserAgent
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# â”€â”€â”€â”€â”€ ë¡œê¹… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger("gsdbg")
def init_logger(level:str):
    logging.basicConfig(
        format="%(asctime)s [%(levelname)5s] %(message)s",
        datefmt="%H:%M:%S", level=getattr(logging, level.upper(), "INFO"))
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("selenium").setLevel(logging.WARNING)

# â”€â”€â”€â”€â”€ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KST = pytz.timezone("Asia/Seoul")
COLS = ["ìˆœìœ„","ìƒí’ˆëª…","ìƒí’ˆID","ì •ê°€","ìµœì¢…ê°€","í• ì¸ìœ¨","ê°€ê²©ë‚´ì—­",
        "íŒë§¤ìëª…","ì¹´í…Œê³ ë¦¬ì½”ë“œ","ì¹´í…Œê³ ë¦¬ë¼ë²¨","ë…¸ì¶œì½”ë“œ",
        "í‰ì ","ë¦¬ë·°ìˆ˜","í”„ë¡œëª¨ì…˜íƒœê·¸","ìˆ˜ì§‘ì‹œê°","ìƒí’ˆURL"]

# â”€â”€â”€â”€â”€ í—¬í¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def txt(el): return el.get_text(strip=True) if el else None
def num_only(s:str|None):
    if not s: return None
    d = re.sub(r"[^\d]","",s); return int(d) if d else None
def nap(base=0.25, jitter=0.2): time.sleep(base + random.random()*jitter)

# â”€â”€â”€â”€â”€ ë“œë¼ì´ë²„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_driver(headless=False):
    ua = UserAgent().chrome
    opts = uc.ChromeOptions()
    if headless: opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--lang=ko-KR")
    opts.add_argument(f"user-agent={ua}")
    drv = uc.Chrome(options=opts, use_subprocess=True)
    stealth(drv, languages=["ko-KR","ko"], vendor="Google Inc.",
            platform="Win32", webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL", fix_hairline=True)
    drv.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument",
        {"source": "Object.defineProperty(navigator,'webdriver',{get:()=>undefined});"})
    logger.debug("Chrome driver ready (headless=%s)", headless)
    return drv

# â”€â”€â”€â”€â”€ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def split_brand_title(raw:str):
    if m := re.match(r"\s*\[([^\]]+)]\s*(.*)", raw):
        return m.group(1), m.group(2).strip()
    parts = raw.split()
    return (parts[0] if parts else ""), raw

def parse(html:str, top:int)->List[Dict[str,Any]]:
    soup  = BeautifulSoup(html,"html.parser")
    nodes = soup.select("section.prd-list li a.prd-item")[:top]
    logger.info("ğŸ” ìµœì¢… íƒìƒ‰ ìƒí’ˆ %dê°œ", len(nodes))
    out=[]
    for idx, a in enumerate(nodes, 1):
        link  = a["href"]
        prdid = a.get("data-prdid")
        brand, title = split_brand_title(txt(a.select_one("dt.prd-name")) or "")
        oldp = num_only(txt(a.select_one("del.price-upper")))
        newp = num_only(txt(a.select_one(".set-price strong") or a.select_one(".set-price")))
        disc = num_only(txt(a.select_one(".price-discount span")))
        rv   = num_only(txt(a.select_one(".user-comment")) or txt(a.select_one(".selling-count")))
        promo = ";".join(t.get_text(strip=True) for t in a.select(".advantage span,.badge-abs span"))
        out.append({
            "ìˆœìœ„": idx, "ìƒí’ˆëª…": title, "ìƒí’ˆID": prdid,
            "ì •ê°€": oldp, "ìµœì¢…ê°€": newp, "í• ì¸ìœ¨": disc, "ê°€ê²©ë‚´ì—­": "",
            "íŒë§¤ìëª…": brand, "ì¹´í…Œê³ ë¦¬ì½”ë“œ": "", "ì¹´í…Œê³ ë¦¬ë¼ë²¨": "", "ë…¸ì¶œì½”ë“œ": "",
            "í‰ì ": None, "ë¦¬ë·°ìˆ˜": rv, "í”„ë¡œëª¨ì…˜íƒœê·¸": promo,
            "ìˆ˜ì§‘ì‹œê°": datetime.datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S"),
            "ìƒí’ˆURL": link,
        })
    return out

# â”€â”€â”€â”€â”€ ìŠ¤í”¼ë„ˆ ëŒ€ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def wait_spinner(driver, timeout=12):
    try:
        t0=time.perf_counter()
        WebDriverWait(driver, 2).until(
            EC.visibility_of_element_located((By.ID, "viewLoading")))
        t1=time.perf_counter()
        WebDriverWait(driver, timeout).until(
            EC.invisibility_of_element_located((By.ID, "viewLoading")))
        t2=time.perf_counter()
        logger.debug("SPINNER show@%.2fs hide@%.2fs  Î”=%.2fs",
                     t1-t0, t2-t0, t2-t1)
    except TimeoutException:
        logger.debug("SPINNER none/timeout")

# â”€â”€â”€â”€â”€ ìŠ¤í¬ë¦°ìƒ· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def snap(driver, directory:Path, tag:str):
    fname = directory/f"{datetime.datetime.now(KST).strftime('%H%M%S')}_{tag}_{uuid.uuid4().hex[:4]}.png"
    driver.save_screenshot(str(fname))

# â”€â”€â”€â”€â”€ ì‚¬ëŒì²˜ëŸ¼ ìŠ¤í¬ë¡¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def human_scroll(driver, top:int, stall_max:int,
                 screenshot_dir:Path|None=None):
    last_len, stall = 0, 0
    logger.info("ğŸš¶â€â™‚ï¸ ìŠ¤í¬ë¡¤ ì‹œì‘ â†’ ëª©í‘œ %dê°œ (stall_max=%d)", top, stall_max)
    i=0
    while True:
        curr = len(driver.find_elements(By.CSS_SELECTOR,
                                        "section.prd-list li a.prd-item"))
        if curr >= top:
            logger.info("âœ… ëª©í‘œ ë‹¬ì„± %d/%dê°œ", curr, top)
            break

        i+=1
        step = random.randint(400, 800)
        driver.execute_script("window.scrollBy(0, arguments[0]);", step)
        y = driver.execute_script("return window.pageYOffset")
        h = driver.execute_script("return document.body.scrollHeight")
        logger.debug("SCROLL #%d +%dpx  y=%d  h=%d", i, step, y, h)

        if screenshot_dir: snap(driver, screenshot_dir, f"{i:02d}")

        nap()
        wait_spinner(driver)
        nap(0.1,0.15)

        new_len = len(driver.find_elements(
                By.CSS_SELECTOR, "section.prd-list li a.prd-item"))
        logger.debug("ITEMS %dâ†’%d (+%d)", curr, new_len, new_len-curr)

        if new_len == last_len:
            stall += 1
            logger.debug("STALL %d/%d", stall, stall_max)
        else:
            stall = 0
        last_len = new_len

        if stall >= stall_max:
            logger.warning("â›” %díšŒ ì—°ì† ì¦ê°€ ç„¡ â€“ ìŠ¤í¬ë¡¤ ì¢…ë£Œ", stall_max)
            break

# â”€â”€â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def crawl(url:str, top:int, headless:bool, out_dir:str,
          stall_max:int, snap_dir:str|None):
    drv = get_driver(headless)
    try:
        logger.info("ğŸŒ GET %s", url)
        drv.get(url)
        wait_spinner(drv)

        sdir = None
        if snap_dir:
            sdir = Path(snap_dir).expanduser().resolve()
            sdir.mkdir(parents=True, exist_ok=True)
            logger.debug("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ê²½ë¡œ: %s", sdir)

        human_scroll(drv, top, stall_max, sdir)
        data = parse(drv.page_source, top)
    finally:
        drv.quit()
        logger.debug("ë“œë¼ì´ë²„ quit")

    df = pd.DataFrame(data)[COLS]
    ts = datetime.datetime.now(KST).strftime("%Y%m%d_%H%M")
    path = Path(out_dir)/f"gs_top{top}_{ts}.xlsx"
    df.to_excel(path, index=False)
    logger.info("[DONE] %dê°œ ì €ì¥ â†’ %s", len(df), path.resolve())

# â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    def_url="https://www.gsshop.com/shop/sect/sectL.gs?sectid=1660575&eh=eyJwYWdlTnVtYmVyIjo3LCJzZWxlY3RlZCI6Im9wdC1wYWdlIiwibHNlY3RZbiI6IlkifQ%3D%3D"
    ap = argparse.ArgumentParser("GS SHOP í¬ë¡¤ëŸ¬ â€“ deep debug")
    ap.add_argument("--list-url", default=def_url)
    ap.add_argument("--top-n", type=int, default=100)
    ap.add_argument("--headless", action="store_true")
    ap.add_argument("--out-dir", default=".")
    ap.add_argument("--stall-max", type=int, default=5,
                    help="ì—°ì† ë¬´ì¦ê°€ í—ˆìš© íšŸìˆ˜(ê¸°ë³¸ 5)")
    ap.add_argument("--snap-dir", default=None,
                    help="DEBUG ëª¨ë“œ ìŠ¤í¬ë¦°ìƒ· ì €ì¥ í´ë”")
    ap.add_argument("--log-level", default="INFO")
    args = ap.parse_args()

    init_logger(args.log_level)
    try:
        crawl(args.list_url, args.top_n, args.headless, args.out_dir,
              args.stall_max, args.snap_dir)
    except KeyboardInterrupt:
        logger.warning("ì‚¬ìš©ì ê°•ì œ ì¢…ë£Œ")
        sys.exit()
