# core/analyzer.py
"""í˜•íƒœì†Œ ë¶„ì„ ëª¨ë“ˆ - NaN ê°’ ì²˜ë¦¬ ê°œì„ """

import pandas as pd
from typing import List
from kiwipiepy import Kiwi
from konlpy.tag import Okt

class MorphologicalAnalyzer:
    """í˜•íƒœì†Œ ë¶„ì„ í´ëž˜ìŠ¤"""
    
    def __init__(self):
        """í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        # Kiwi ì´ˆê¸°í™” (ê¸°ë³¸)
        self.kiwi = Kiwi()
        
        # KoNLPy ì´ˆê¸°í™”
        self.okt = Okt()
        print("âœ… í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ (Kiwi + KoNLPy)")
    
    def tokenize_with_kiwi(self, text: str, pos_filter: List[str] = ['NNG', 'NNP']) -> List[str]:
        """Kiwië¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„ - NaN ê°’ ì²˜ë¦¬ ê°œì„ """
        # NaN ê°’ì´ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if pd.isna(text) or not text or text == '':
            return []
        
        # ë¬¸ìžì—´ë¡œ ë³€í™˜
        text = str(text).strip()
        if not text:
            return []
        
        try:
            tokens = self.kiwi.tokenize(text)
            filtered_tokens = []
            
            for token in tokens:
                if token.tag in pos_filter and len(token.form) > 1:
                    filtered_tokens.append(token.form)
            
            return filtered_tokens
        except Exception as e:
            print(f"âš ï¸ Kiwi í† í°í™” ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text[:50]}...'): {e}")
            return []
    
    def tokenize_with_okt(self, text: str, pos_filter: List[str] = ['Noun', 'Verb', 'Adjective']) -> List[str]:
        """OKTë¥¼ ì´ìš©í•œ í˜•íƒœì†Œ ë¶„ì„ - NaN ê°’ ì²˜ë¦¬ ê°œì„ """
        # NaN ê°’ì´ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if pd.isna(text) or not text or text == '':
            return []
        
        # ë¬¸ìžì—´ë¡œ ë³€í™˜
        text = str(text).strip()
        if not text:
            return []
        
        try:
            tokens = self.okt.pos(text, stem=True)
            filtered_tokens = []
            
            for word, pos in tokens:
                if pos in pos_filter and len(word) > 1:
                    filtered_tokens.append(word)
                    
            return filtered_tokens
        except Exception as e:
            print(f"âš ï¸ OKT í† í°í™” ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text[:50]}...'): {e}")
            return []
    
    def tokenize_dataframe(self, df: pd.DataFrame, text_column: str, 
                          method: str = 'kiwi') -> pd.DataFrame:
        """DataFrameì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ í† í°í™” - ê°œì„ ëœ ë²„ì „"""
        print(f"ðŸ”¤ {method}ë¡œ ë¦¬ë·° í† í°í™” ì¤‘...")
        
        df = df.copy()
        
        # NaN ê°’ê³¼ ë¹ˆ ê°’ ì‚¬ì „ ì²˜ë¦¬
        print(f"ðŸ“Š í† í°í™” ì „ ë°ì´í„° ìƒíƒœ:")
        print(f"   - ì „ì²´ í–‰ ìˆ˜: {len(df)}")
        print(f"   - {text_column} ì»¬ëŸ¼ NaN ê°œìˆ˜: {df[text_column].isna().sum()}")
        print(f"   - {text_column} ì»¬ëŸ¼ ë¹ˆ ë¬¸ìžì—´ ê°œìˆ˜: {(df[text_column] == '').sum()}")
        
        # NaN ê°’ì„ ë¹ˆ ë¬¸ìžì—´ë¡œ ë³€í™˜
        df[text_column] = df[text_column].fillna('')
        
        # ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìžˆëŠ” í–‰ë§Œ í•„í„°ë§
        valid_mask = df[text_column].astype(str).str.strip().str.len() > 0
        original_count = len(df)
        df = df[valid_mask].copy()
        filtered_count = len(df)
        
        if filtered_count < original_count:
            print(f"âš ï¸ ë¹ˆ í…ìŠ¤íŠ¸ë¡œ ì¸í•´ {original_count - filtered_count}ê°œ í–‰ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        if len(df) == 0:
            print("âŒ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        # í† í°í™” ì‹¤í–‰
        if method == 'okt':
            df['tokens'] = df[text_column].apply(self.tokenize_with_okt)
        else:  # ê¸°ë³¸ê°’ kiwi
            df['tokens'] = df[text_column].apply(self.tokenize_with_kiwi)
        
        # í† í°í™”ëœ ê²°ê³¼ë¥¼ ë¬¸ìžì—´ë¡œ ë³€í™˜
        df['tokens_str'] = df['tokens'].apply(lambda x: ' '.join(x) if x else '')
        
        # í† í°ì´ ì—†ëŠ” í–‰ ì œê±°
        empty_tokens_mask = df['tokens'].apply(len) > 0
        before_token_filter = len(df)
        df = df[empty_tokens_mask].copy()
        after_token_filter = len(df)
        
        if after_token_filter < before_token_filter:
            print(f"âš ï¸ í† í°ì´ ì—†ëŠ” {before_token_filter - after_token_filter}ê°œ í–‰ì´ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… í† í°í™” ì™„ë£Œ: {len(df)}ê°œ ë¦¬ë·°")
        print(f"ðŸ“Š í† í°í™” ê²°ê³¼:")
        print(f"   - í‰ê·  í† í° ìˆ˜: {df['tokens'].apply(len).mean():.1f}ê°œ")
        print(f"   - ìµœëŒ€ í† í° ìˆ˜: {df['tokens'].apply(len).max()}ê°œ")
        
        return df