# keywords/extractor.py
"""í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë“ˆ - ì˜ë¯¸ ë‹¨ìœ„ êµ¬ë¬¸ ì¤‘ì‹¬"""

import pandas as pd
import numpy as np
from typing import List, Tuple, Dict
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

class KeywordExtractor:
    """í‚¤ì›Œë“œ ì¶”ì¶œ í´ë˜ìŠ¤ - êµ¬ë¬¸ ì¤‘ì‹¬"""
    
    def __init__(self):
        """í‚¤ì›Œë“œ ì¶”ì¶œê¸° ì´ˆê¸°í™”"""
        self.auto_stopwords = set()
    
    def extract_auto_stopwords(self, texts: List[str], threshold: float = 0.7) -> set:
        """
        ìë™ ë¶ˆìš©ì–´ ì¶”ì¶œ - ë„ˆë¬´ í”í•œ ë‹¨ì–´ë“¤
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            threshold: ë¶ˆìš©ì–´ íŒë‹¨ ì„ê³„ê°’ (ë¬¸ì„œ ë¹ˆë„ ë¹„ìœ¨)
            
        Returns:
            ì¶”ì¶œëœ ë¶ˆìš©ì–´ ì§‘í•©
        """
        try:
            vectorizer = TfidfVectorizer(
                tokenizer=lambda x: x.split(),
                lowercase=False,
                min_df=2
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # ë¬¸ì„œ ë¹ˆë„ ê³„ì‚°
            doc_freq = np.array((tfidf_matrix > 0).sum(axis=0)).flatten()
            total_docs = len(texts)
            doc_freq_ratio = doc_freq / total_docs
            
            # ë„ˆë¬´ í”í•œ ë‹¨ì–´ë“¤ì„ ë¶ˆìš©ì–´ë¡œ íŒì •
            too_common = set(feature_names[doc_freq_ratio > threshold])
            
            self.auto_stopwords = too_common
            print(f"ğŸ“Š ìë™ ë¶ˆìš©ì–´ ì¶”ì¶œ: {len(self.auto_stopwords)}ê°œ")
            
            return self.auto_stopwords
            
        except Exception as e:
            print(f"ìë™ ë¶ˆìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return set()
    
    def extract_meaningful_phrases(self, texts: List[str], min_freq: int = 3, 
                                 max_ngram: int = 3) -> List[Tuple[str, float]]:
        """
        ì˜ë¯¸ìˆëŠ” êµ¬ë¬¸ ì¶”ì¶œ (N-gram ê¸°ë°˜)
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            min_freq: ìµœì†Œ ì¶œí˜„ ë¹ˆë„
            max_ngram: ìµœëŒ€ n-gram ê¸¸ì´
            
        Returns:
            (êµ¬ë¬¸, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ” ì˜ë¯¸ìˆëŠ” êµ¬ë¬¸ ì¶”ì¶œ ì¤‘...")
        
        all_ngrams = []
        
        for text in texts:
            words = text.split()
            # 1-gramë¶€í„° max_ngramê¹Œì§€ ìƒì„±
            for n in range(1, max_ngram + 1):
                for i in range(len(words) - n + 1):
                    ngram = ' '.join(words[i:i+n])
                    
                    # ë¶ˆìš©ì–´ í•„í„°ë§
                    if not any(word in self.auto_stopwords for word in ngram.split()):
                        # ê¸¸ì´ í•„í„°ë§
                        if 2 <= len(ngram.replace(' ', '')) <= 20:
                            all_ngrams.append(ngram)
        
        # ë¹ˆë„ ê³„ì‚°
        ngram_counts = Counter(all_ngrams)
        
        # ìµœì†Œ ë¹ˆë„ ì´ìƒë§Œ ì„ íƒ
        frequent_ngrams = {ngram: count for ngram, count in ngram_counts.items() 
                          if count >= min_freq}
        
        # ì ìˆ˜ ê³„ì‚°: ë¹ˆë„ Ã— ê¸¸ì´ ê°€ì¤‘ì¹˜
        scored_phrases = []
        for ngram, freq in frequent_ngrams.items():
            word_count = len(ngram.split())
            # ê¸´ êµ¬ë¬¸ì— ê°€ì¤‘ì¹˜, í•˜ì§€ë§Œ ê³¼ë„í•˜ì§€ ì•Šê²Œ
            score = freq * (1 + word_count * 0.3)
            scored_phrases.append((ngram, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_phrases.sort(key=lambda x: x[1], reverse=True)
        
        print(f"âœ… {len(scored_phrases)}ê°œ ì˜ë¯¸êµ¬ë¬¸ ì¶”ì¶œ ì™„ë£Œ")
        return scored_phrases
    
    def cluster_based_topics(self, df: pd.DataFrame, text_col: str = 'tokens_str', 
                           n_clusters: int = 5) -> Tuple[Dict, pd.DataFrame]:
        """
        í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ í† í”½ ì¶”ì¶œ
        
        Args:
            df: ì…ë ¥ DataFrame
            text_col: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            n_clusters: í´ëŸ¬ìŠ¤í„° ìˆ˜
            
        Returns:
            (í† í”½ ë”•ì…”ë„ˆë¦¬, í´ëŸ¬ìŠ¤í„° ë¼ë²¨ì´ ì¶”ê°€ëœ DataFrame)
        """
        print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ {n_clusters}ê°œ í† í”½ ì¶”ì¶œ ì¤‘...")
        
        texts = df[text_col].tolist()
        
        try:
            # TF-IDF ë²¡í„°í™” (ìë™ ë¶ˆìš©ì–´ ì œì™¸)
            stop_words = list(self.auto_stopwords) if self.auto_stopwords else None
            
            vectorizer = TfidfVectorizer(
                max_features=200,
                token_pattern=r'\b\w+\b',
                min_df=2,
                max_df=0.8,
                stop_words=stop_words
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)
            
            # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            topics = {}
            for i in range(n_clusters):
                center = kmeans.cluster_centers_[i]
                top_indices = center.argsort()[-10:][::-1]
                top_keywords = [feature_names[idx] for idx in top_indices]
                
                cluster_size = sum(cluster_labels == i)
                ratio = cluster_size / len(texts)
                
                topics[f'í† í”½_{i+1}'] = {
                    'keywords': top_keywords,
                    'size': cluster_size,
                    'ratio': ratio
                }
            
            # DataFrameì— í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
            df_with_clusters = df.copy()
            df_with_clusters['topic_cluster'] = cluster_labels
            
            print(f"âœ… í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í† í”½ ì¶”ì¶œ ì™„ë£Œ")
            for topic, info in topics.items():
                print(f"   {topic}: {info['size']}ê°œ ({info['ratio']:.1%})")
            
            return topics, df_with_clusters
            
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ í† í”½ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}, df.copy()
    
    def extract_keywords_tfidf(self, df: pd.DataFrame, text_column: str = 'tokens_str',
                              max_features: int = 100) -> List[Tuple[str, float]]:
        """
        TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ë¨)
        
        Args:
            df: ì…ë ¥ DataFrame
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            max_features: ìµœëŒ€ íŠ¹ì„± ìˆ˜
            
        Returns:
            (í‚¤ì›Œë“œ, ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        print("ğŸ“Š TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        # ìë™ ë¶ˆìš©ì–´ ì œì™¸
        stop_words = list(self.auto_stopwords) if self.auto_stopwords else None
        
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),  # 1-2 gram
            token_pattern=r'\b\w+\b',
            stop_words=stop_words
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(df[text_column])
            feature_names = vectorizer.get_feature_names_out()
            
            # ì „ì²´ TF-IDF ì ìˆ˜ í•©ê³„
            tfidf_scores = tfidf_matrix.sum(axis=0).A1
            keyword_scores = list(zip(feature_names, tfidf_scores))
            keyword_scores.sort(key=lambda x: x[1], reverse=True)
            
            print(f"âœ… TF-IDF í‚¤ì›Œë“œ {len(keyword_scores)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            return keyword_scores
            
        except Exception as e:
            print(f"âŒ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_sentiment_keywords(self, df: pd.DataFrame, sentiment_column: str = 'sentiment',
                                  text_column: str = 'tokens_str', top_n: int = 15) -> Dict[str, List[Tuple[str, float]]]:
        """
        ê°ì„±ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            df: ì…ë ¥ DataFrame
            sentiment_column: ê°ì„± ì»¬ëŸ¼ëª…
            text_column: í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…
            top_n: ìƒìœ„ nê°œ í‚¤ì›Œë“œ
            
        Returns:
            ê°ì„±ë³„ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬
        """
        print("ğŸ’­ ê°ì„±ë³„ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        
        sentiment_keywords = {}
        
        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_data = df[df[sentiment_column] == sentiment]
            
            if len(sentiment_data) > 0:
                sentiment_texts = sentiment_data[text_column].tolist()
                combined_text = ' '.join(sentiment_texts)
                
                # í•´ë‹¹ ê°ì„±ì˜ íŠ¹ì§• í‚¤ì›Œë“œ ì¶”ì¶œ
                vectorizer = TfidfVectorizer(
                    max_features=top_n,
                    ngram_range=(1, 2),
                    token_pattern=r'\b\w+\b',
                    stop_words=list(self.auto_stopwords) if self.auto_stopwords else None
                )
                
                try:
                    tfidf_matrix = vectorizer.fit_transform([combined_text])
                    feature_names = vectorizer.get_feature_names_out()
                    tfidf_scores = tfidf_matrix.toarray()[0]
                    
                    keywords = sorted(zip(feature_names, tfidf_scores), 
                                    key=lambda x: x[1], reverse=True)
                    sentiment_keywords[sentiment] = keywords
                    
                except Exception:
                    sentiment_keywords[sentiment] = []
            else:
                sentiment_keywords[sentiment] = []
        
        return sentiment_keywords