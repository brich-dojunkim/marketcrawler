# -*- coding: utf-8 -*-
"""ë„¤ì´ë²„ ë¸Œëœë“œìŠ¤í† ì–´ ìƒí’ˆ ë¦¬ë·° ì¼ê´„ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
   naver_brandstore.py ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ìƒí’ˆì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘
"""

from __future__ import annotations

import argparse
import csv
import logging
import random
import re
import sys
import time
from pathlib import Path
from typing import Iterable, List, Tuple

import pandas as pd

# naver_review.pyì—ì„œ í¬ë¡¤ëŸ¬ ì„í¬íŠ¸
try:
    from naver_review import NaverReviewCrawler, NaverReview
except ImportError:
    print("naver_review.py íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    sys.exit(1)

###############################################################################
# ë¡œê¹… ì„¤ì •
###############################################################################
log = logging.getLogger("naver_bulk_reviews")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)-7s | %(message)s",
)

###############################################################################
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
###############################################################################

def slugify(text: str, maxlen: int = 40) -> str:
    """ì•ˆì „í•œ íŒŒì¼ëª…ì„ ìœ„í•œ ìŠ¬ëŸ¬ê·¸ ìƒì„±"""
    s = re.sub(r'[^\w\-]+', '-', text.strip().lower())
    return s[:maxlen].strip('-') or 'item'

def extract_representative_title(src: str | float) -> str:
    """ìƒí’ˆëª…ì—ì„œ ëŒ€í‘œ ì œëª© ì¶”ì¶œ"""
    if not isinstance(src, str):
        return "ìƒí’ˆ"
    return src.split(',')[0].strip()

def deduplicate_urls(rows: Iterable[Tuple[str, str]]) -> List[Tuple[str, str]]:
    """URL ì¤‘ë³µ ì œê±°"""
    seen: set[str] = set()
    unique: list[Tuple[str, str]] = []
    
    for name, url in rows:
        # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•œ ë² ì´ìŠ¤ URLë¡œ ì¤‘ë³µ ì²´í¬
        base = url.split('?', 1)[0]
        if base in seen:
            continue
        seen.add(base)
        unique.append((name, url))
    
    return unique

def load_product_urls(src: str, limit: int | None = None) -> List[Tuple[str, str]]:
    """CSV/JSONì—ì„œ ìƒí’ˆ URL ëª©ë¡ ë¡œë“œ"""
    try:
        if src.endswith('.csv'):
            df = pd.read_csv(src)
        elif src.endswith('.json'):
            df = pd.read_json(src)
        else:
            raise ValueError("ì§€ì› í˜•ì‹: csv | json")

        if 'product_url' not in df.columns:
            raise KeyError("product_url ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")

        if limit:
            df = df.head(limit)
            log.info(f"ìƒìœ„ {limit}ê°œ ìƒí’ˆë§Œ ëŒ€ìƒ")

        # ìƒí’ˆëª… ì»¬ëŸ¼ ì°¾ê¸°
        name_col = 'name' if 'name' in df.columns else df.columns[0]

        raw_data = [
            (extract_representative_title(row.get(name_col, "")), str(row["product_url"]))
            for _, row in df.iterrows()
            if pd.notna(row.get("product_url"))
        ]

        return deduplicate_urls(raw_data)

    except Exception as e:
        log.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

def save_individual_reviews(reviews: List[NaverReview], base_out: Path, item_slug: str, fmt: str):
    """ê°œë³„ ìƒí’ˆ ë¦¬ë·° ì €ì¥"""
    if not reviews:
        log.info("    â†³ ë¦¬ë·° 0ê±´ â€“ ìŠ¤í‚µ")
        return

    base_out.mkdir(parents=True, exist_ok=True)
    ts = time.strftime("%Y%m%d_%H%M%S")
    filename = base_out / f"naver_reviews_{item_slug}_{ts}.{fmt}"

    df = pd.DataFrame.from_records([r.__dict__ for r in reviews])
    
    if fmt == "csv":
        df.to_csv(filename, index=False, encoding='utf-8-sig', quoting=csv.QUOTE_MINIMAL)
    else:
        df.to_json(filename, orient="records", force_ascii=False, indent=2)

    log.info(f"    â†³ ì €ì¥: {filename} ({len(df)}ê±´)")

###############################################################################
# ë©”ì¸ í¬ë¡¤ë§ ë¡œì§
###############################################################################

def crawl_product_reviews(
    name: str, 
    url: str, 
    pages: int, 
    sort_option: str,
    headless: int, 
    fmt: str, 
    outdir: Path
) -> List[NaverReview]:
    """ë‹¨ì¼ ìƒí’ˆ ë¦¬ë·° í¬ë¡¤ë§"""
    
    item_slug = slugify(name)
    
    try:
        crawler = NaverReviewCrawler(
            url=url,
            product_name=name,
            pages=pages,
            sort_option=sort_option,
            headless=headless,
            fmt=fmt,
            outdir=outdir,
        )
        
        reviews = crawler.run()
        
        # ê°œë³„ ì €ì¥
        save_individual_reviews(reviews, outdir, item_slug, fmt)
        
        return reviews
        
    except Exception as e:
        log.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return []

###############################################################################
# CLI
###############################################################################

def main(argv: List[str] | None = None):
    parser = argparse.ArgumentParser(
        description="ë„¤ì´ë²„ ë¸Œëœë“œìŠ¤í† ì–´ ìƒí’ˆ ë¦¬ë·° ì¼ê´„ í¬ë¡¤ëŸ¬ â€“ CSV/JSON ê¸°ë°˜"
    )
    parser.add_argument("src", help="naver_brandstore ê²°ê³¼ íŒŒì¼ (csv/json)")
    parser.add_argument("--pages", type=int, default=5, help="í˜ì´ì§€ë‹¹ ë¦¬ë·° í˜ì´ì§€ ìˆ˜")
    parser.add_argument("--sort", default="ë­í‚¹ìˆœ", 
                       choices=["ë­í‚¹ìˆœ", "ìµœì‹ ìˆœ", "í‰ì  ë†’ì€ìˆœ", "í‰ì  ë‚®ì€ìˆœ"],
                       help="ë¦¬ë·° ì •ë ¬ ì˜µì…˜")
    parser.add_argument("--headless", type=int, default=1, choices=[0, 1])
    parser.add_argument("--fmt", choices=["csv", "json"], default="csv")
    parser.add_argument("--outdir", default="./output/naver_reviews")
    parser.add_argument("--limit", type=int, help="ì²˜ë¦¬í•  ìƒí’ˆ ìˆ˜ ì œí•œ")
    parser.add_argument("--delay", type=float, default=3.0, help="ìƒí’ˆ ê°„ ëŒ€ê¸°ì‹œê°„(ì´ˆ)")
    parser.add_argument("--retry", type=int, default=2, help="ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜")
    
    args = parser.parse_args(argv)

    # ìƒí’ˆ URL ëª©ë¡ ë¡œë“œ
    products = load_product_urls(args.src, args.limit)
    total = len(products)
    
    if not products:
        log.error("ì²˜ë¦¬í•  ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    log.info(f"ì´ {total}ê°œ ìƒí’ˆ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘")
    log.info(f"ì„¤ì •: í˜ì´ì§€={args.pages}, ì •ë ¬={args.sort}, í—¤ë“œë¦¬ìŠ¤={args.headless}")

    # í†µê³„ ë³€ìˆ˜
    success_count = 0
    fail_count = 0
    total_reviews = 0
    
    # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    outdir = Path(args.outdir)
    outdir.mkdir(parents=True, exist_ok=True)

    # ê° ìƒí’ˆë³„ í¬ë¡¤ë§
    for idx, (name, url) in enumerate(products, 1):
        log.info(f"\n[{idx}/{total}] {name[:60]}...")
        log.info(f"    URL: {url}")
        
        retry_count = 0
        reviews = []
        
        # ì¬ì‹œë„ ë¡œì§
        while retry_count <= args.retry:
            try:
                reviews = crawl_product_reviews(
                    name=name,
                    url=url,
                    pages=args.pages,
                    sort_option=args.sort,
                    headless=args.headless,
                    fmt=args.fmt,
                    outdir=outdir
                )
                
                if reviews:
                    success_count += 1
                    total_reviews += len(reviews)
                    log.info(f"    âœ… ì„±ê³µ: {len(reviews)}ê°œ ë¦¬ë·° ìˆ˜ì§‘")
                    break
                else:
                    log.warning(f"    âš ï¸ ë¦¬ë·° 0ê±´")
                    break
                    
            except Exception as e:
                retry_count += 1
                log.error(f"    âŒ ì‹œë„ {retry_count} ì‹¤íŒ¨: {e}")
                
                if retry_count <= args.retry:
                    log.info(f"    ğŸ”„ {retry_count + 1}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(retry_count + 1)
                else:
                    fail_count += 1
                    log.error(f"    âŒ ìµœì¢… ì‹¤íŒ¨: {name}")
                    break
        
        # ìƒí’ˆ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ ìƒí’ˆì´ ì•„ë‹Œ ê²½ìš°)
        if idx < total:
            delay = random.uniform(args.delay, args.delay + 1.5)
            log.info(f"    â³ {delay:.1f}ì´ˆ ëŒ€ê¸°...")
            time.sleep(delay)

    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    log.info(f"\n" + "="*50)
    log.info(f"í¬ë¡¤ë§ ì™„ë£Œ!")
    log.info(f"  â€¢ ì´ ìƒí’ˆ: {total}ê°œ")
    log.info(f"  â€¢ ì„±ê³µ: {success_count}ê°œ")
    log.info(f"  â€¢ ì‹¤íŒ¨: {fail_count}ê°œ")
    log.info(f"  â€¢ ìˆ˜ì§‘ëœ ë¦¬ë·°: {total_reviews:,}ê°œ")
    log.info(f"  â€¢ ì„±ê³µë¥ : {success_count/total*100:.1f}%")
    log.info(f"  â€¢ ê²°ê³¼ ìœ„ì¹˜: {outdir}")
    log.info(f"="*50)

    # ìš”ì•½ í†µê³„ íŒŒì¼ ì €ì¥
    summary = {
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
        'total_products': total,
        'success_count': success_count,
        'fail_count': fail_count,
        'total_reviews': total_reviews,
        'success_rate': round(success_count/total*100, 2) if total > 0 else 0,
        'settings': {
            'pages': args.pages,
            'sort': args.sort,
            'headless': args.headless,
            'format': args.fmt
        }
    }
    
    summary_file = outdir / f"crawling_summary_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        import json
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    log.info(f"ìš”ì•½ íŒŒì¼ ì €ì¥: {summary_file}")


if __name__ == "__main__":
    main(sys.argv[1:])