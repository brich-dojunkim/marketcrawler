# cjonstyle.py  ‚Äì  CJÏò®Ïä§ÌÉÄÏùº Ïó¨ÏÑ±ÏùòÎ•ò Top-100 ÌÅ¨Î°§Îü¨
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  ‚Ä¢ URL¬∑Í∞úÏàò¬∑Ìó§ÎìúÎ¶¨Ïä§ Î™®Îìú Î™®Îëê ÏΩîÎìú ÏïàÏóê Í∏∞Î≥∏Í∞íÏúºÎ°ú Í≥†Ï†ï
#  ‚Ä¢ Í≤∞Í≥º Ïó¥ ÏàúÏÑú :  ÏàúÏúÑ, ÏÉÅÌíàÎ™Ö, Î∏åÎûúÎìúÎ™Ö, ÏÉÅÌíàID, Ï†ïÍ∞Ä, ÏµúÏ¢ÖÍ∞Ä, Ìï†Ïù∏Ïú®,
#                  Í∞ÄÍ≤©ÎÇ¥Ïó≠, ÌòúÌÉù, Î¶¨Î∑∞/Íµ¨Îß§Ïàò, ÏàòÏßëÏãúÍ∞Å, ÏÉÅÌíàURL
#  ‚Ä¢ SSL¬∑ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò¬∑ÏûêÏó∞Ïä§ÌÅ¨Î°§ Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
# ------------------------------------------------------
from __future__ import annotations
import dataclasses as dc, re, ssl, time, logging, datetime as dt, urllib.request
from pathlib import Path
from typing import List, Optional

import certifi, pandas as pd, undetected_chromedriver as uc
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Ï†ÑÏó≠ ÏÑ§Ï†ï ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TARGET_URL  = "https://display.cjonstyle.com/p/category/categoryMain?dpCateId=G00011"
MAX_PAGES   = 10          # ÌéòÏù¥ÏßÄÎÑ§Ïù¥ÏÖò ÏÉÅÌïú
TARGET_EACH = 120         # ÌéòÏù¥ÏßÄÎãπ Ïä§ÌÅ¨Î°§ Î™©Ìëú
LIMIT       = 100         # Ï¥ù ÏàòÏßë Í∞úÏàò
OUT_DIR     = Path("output")
HEADLESS    = True        # ÌôîÎ©¥ Ïïà ÎùÑÏö∞Í∏∞

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SSL Ïö∞Ìöå (certifi) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ctx = ssl.create_default_context(cafile=certifi.where())
urllib.request.install_opener(
    urllib.request.build_opener(urllib.request.HTTPSHandler(context=ctx))
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Ìó¨Ìçº Ìï®Ïàò ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def digits(text: str) -> Optional[int]:
    m = re.sub(r"[^\d]", "", text or "")
    return int(m) if m else None

def join_or_blank(parts: List[str]) -> str:
    return ";".join(p for p in parts if p)

def split_brand_title(raw: str) -> tuple[str, str]:
    """'[Î∏åÎûúÎìú] ÏÉÅÌíàÎ™Ö' ÎòêÎäî 'Î∏åÎûúÎìú ÏÉÅÌíàÎ™Ö' Ìå®ÌÑ¥ Î∂ÑÎ¶¨"""
    if m := re.match(r"\s*\[([^\]]+)\]\s*(.*)", raw):
        return m.group(1), m.group(2).strip()
    parts = raw.split(maxsplit=1)
    if len(parts) == 2:
        return parts[0], parts[1]
    return "", raw

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Îç∞Ïù¥ÌÑ∞ ÌÅ¥ÎûòÏä§ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@dc.dataclass
class Product:
    rank: int
    name: str
    brand: str
    item_id: str
    list_price: Optional[int]
    sale_price: Optional[int]
    discount: Optional[int]
    price_detail: str
    benefit: str
    review_cnt: Optional[int]
    collected_at: str
    url: str

    @classmethod
    def from_li(cls, li, rank: int, ts: str) -> "Product":
        # ÌÖçÏä§Ìä∏ ÏïàÏ†Ñ Ï∂îÏ∂ú
        def txt(css: str) -> str:
            try:
                return li.find_element(By.CSS_SELECTOR, css).text.strip()
            except:
                return ""

        # a.gaclass ÏöîÏÜå
        a_item = li.find_element(By.CSS_SELECTOR, "a.gaclass")

        # ÏÉÅÌíàIDÎ•º a.gaclass ÏÜçÏÑ±ÏóêÏÑú Í∞ÄÏ†∏Ïò§Í∏∞
        item_id = (
            a_item.get_attribute("data-dqid")
            or a_item.get_attribute("data-item-cd")
            or ""
        )

        # ÏõêÎ≥∏ Ïù¥Î¶Ñ & Î∏åÎûúÎìú/ÏÉÅÌíàÎ™Ö Î∂ÑÎ¶¨
        raw_name = txt("strong.goods_name")
        brand_txt, title = split_brand_title(raw_name)
        brand = txt(".shop_area a.goBrand") or brand_txt

        # Í∞ÄÍ≤© Ï†ïÎ≥¥
        sale   = digits(txt(".wrap_price ins .num"))
        normal = digits(txt(".wrap_price del .num"))
        disc   = digits(txt(".discount .num"))
        price_detail = join_or_blank([
            f"Ìï†Ïù∏Í∞Ä{sale:,}Ïõê~"   if sale   else "",
            f"Ï†ïÏÉÅÍ∞Ä{normal:,}Ïõê~" if normal else "",
        ])

        # ÌòúÌÉù Ï†ïÎ≥¥
        benefits = [b.text.strip() for b in li.find_elements(By.CSS_SELECTOR, ".info_benefit .benefit_item")]
        if li.find_elements(By.CSS_SELECTOR, ".shop_area .ico_free"):
            benefits.append("Î¨¥Î£åÎ∞∞ÏÜ°")
        benefit = join_or_blank(benefits)

        # Î¶¨Î∑∞/Íµ¨Îß§Ïàò
        review_cnt = digits(txt(".customer_count .num"))

        # ÎßÅÌÅ¨
        link = a_item.get_attribute("href") or ""

        return cls(
            rank        = rank,
            name        = title,
            brand       = brand,
            item_id     = item_id,
            list_price  = normal,
            sale_price  = sale,
            discount    = disc,
            price_detail= price_detail,
            benefit     = benefit,
            review_cnt  = review_cnt,
            collected_at= ts,
            url         = link,
        )

    def to_record(self) -> dict:
        return {
            "ÏàúÏúÑ"       : self.rank,
            "ÏÉÅÌíàÎ™Ö"     : self.name,
            "Î∏åÎûúÎìúÎ™Ö"   : self.brand,
            "ÏÉÅÌíàID"     : self.item_id,
            "Ï†ïÍ∞Ä"       : self.list_price,
            "ÏµúÏ¢ÖÍ∞Ä"     : self.sale_price,
            "Ìï†Ïù∏Ïú®"     : self.discount,
            "Í∞ÄÍ≤©ÎÇ¥Ïó≠"   : self.price_detail,
            "ÌòúÌÉù"       : self.benefit,
            "Î¶¨Î∑∞/Íµ¨Îß§Ïàò" : self.review_cnt,
            "ÏàòÏßëÏãúÍ∞Å"   : self.collected_at,
            "ÏÉÅÌíàURL"    : self.url,
        }

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Selenium ÎìúÎùºÏù¥Î≤Ñ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_driver() -> webdriver.Chrome:
    opt = uc.ChromeOptions()
    if HEADLESS:
        opt.add_argument("--headless=new")
    opt.add_argument("--no-sandbox")
    opt.add_argument("--disable-gpu")
    opt.add_argument("--lang=ko-KR")
    return uc.Chrome(options=opt)

def smooth_scroll(driver, want: int, pause: float = .4):
    last = 0; stagnant = 0
    while True:
        driver.execute_script("window.scrollBy(0,800)")
        time.sleep(pause)
        count = len(driver.find_elements(By.CSS_SELECTOR, "ul.lst_cate_result>li"))
        if count >= want:
            break
        cur = driver.execute_script("return document.documentElement.scrollTop+window.innerHeight")
        if cur == last:
            stagnant += 1
            if stagnant >= 5:
                break
        else:
            stagnant = 0; last = cur

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Î©îÏù∏ Î£®ÌîÑ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def crawl():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s %(message)s", datefmt="%H:%M:%S")
    OUT_DIR.mkdir(exist_ok=True)
    drv = build_driver()
    ts = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    products: List[Product] = []

    try:
        drv.get(TARGET_URL)
        page = 1
        while True:
            WebDriverWait(drv, 20).until(
                EC.presence_of_all_elements_located((By.CSS_SELECTOR, "ul.lst_cate_result>li"))
            )
            need = min(TARGET_EACH, LIMIT - len(products))
            smooth_scroll(drv, need)
            lis = drv.find_elements(By.CSS_SELECTOR, "ul.lst_cate_result>li")[:need]
            for li in lis:
                products.append(Product.from_li(li, len(products)+1, ts))
                if len(products) >= LIMIT:
                    break
            logging.info("üìÑ page %d ‚Äì ÏàòÏßë %dÍ∞ú", page, len(products))
            if len(products) >= LIMIT or page >= MAX_PAGES:
                break
            try:
                nxt = drv.find_element(By.CSS_SELECTOR, "a.btn_pn_next:not(.disabled)")
                drv.execute_script("arguments[0].click()", nxt)
            except:
                break
            page += 1
            time.sleep(1)
    finally:
        drv.quit()

    # Excel Ï†ÄÏû•
    df = pd.DataFrame([p.to_record() for p in products])
    fname = OUT_DIR / f"cj_{dt.datetime.now():%Y%m%d_%H%M}.xlsx"
    df.to_excel(fname, index=False)
    logging.info("[DONE] Ï¥ù %dÍ∞ú Ï†ÄÏû• ‚Üí %s", len(df), fname.resolve())

if __name__ == "__main__":
    crawl()
